-- ========================================
-- SCRIPT COMPLET : 28 TUTORIELS PROFESSIONNELS
-- DevOps, Cloud, Kubernetes, CI/CD, Terraform, Ansible, Monitoring, Automation
-- ========================================

-- IMPORTANT: Remplacez '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3' par votre user_id r√©el
-- Pour trouver votre user_id : SELECT id FROM auth.users LIMIT 1;

-- ========================================
-- √âTAPE 1 : NETTOYAGE COMPLET
-- ========================================

DELETE FROM blog_posts;

-- ========================================
-- CAT√âGORIE : CLOUD (4 tutoriels)
-- ========================================

-- CLOUD 1: AWS Architecture 3-Tiers
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'AWS : D√©ployer une Architecture 3-Tiers Scalable',
    'aws-architecture-3-tiers',
    $BODY$# AWS : Architecture 3-Tiers Production-Ready

## üéØ Use Case : Application Web Scalable

Startup qui passe de 1000 √† 1 million d'utilisateurs. Architecture : Load Balancer ‚Üí Serveurs Web (Auto-Scaling) ‚Üí Base de donn√©es (Multi-AZ).

## Architecture

```
Internet ‚Üí CloudFront (CDN) ‚Üí ALB ‚Üí EC2 Auto-Scaling Group ‚Üí RDS Multi-AZ
```

## √âtape 1 : VPC et Subnets

```hcl
resource "aws_vpc" "main" {
  cidr_block           = "10.0.0.0/16"
  enable_dns_hostnames = true

  tags = {
    Name = "production-vpc"
  }
}

resource "aws_subnet" "public" {
  count = 3
  vpc_id                  = aws_vpc.main.id
  cidr_block              = "10.0.${count.index + 1}.0/24"
  availability_zone       = data.aws_availability_zones.available.names[count.index]
  map_public_ip_on_launch = true
}
```

## ROI

- **Haute Disponibilit√©** : 99.99% uptime
- **Scalabilit√©** : 2 ‚Üí 100 instances automatiquement
- **Co√ªts** : Pay-as-you-go, -40% vs serveurs d√©di√©s$BODY$,
    'D√©ployez une architecture AWS 3-tiers scalable avec Terraform. VPC, ALB, Auto-Scaling, RDS Multi-AZ. Production-ready avec haute disponibilit√©.',
    '/images/tutorials/cloud-aws.svg',
    'Cloud',
    ARRAY['AWS', 'Cloud', 'Terraform', '3-Tiers', 'Architecture', 'Scalability'],
    'published',
    NOW() - INTERVAL '90 days',
    145,
    23,
    'AWS Architecture 3-Tiers : Scalable et Haute Disponibilit√©',
    'Architecture AWS 3-tiers avec Terraform. VPC, ALB, Auto-Scaling, RDS Multi-AZ. Production-ready 99.99% uptime.',
    ARRAY['aws', 'cloud', 'terraform', '3-tiers', 'architecture', 'scalability']
);

-- CLOUD 2: Azure DevOps + AKS
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Azure : Pipeline DevOps Complet avec AKS et ACR',
    'azure-devops-aks-pipeline',
    $BODY$# Azure DevOps + AKS : CI/CD Cloud-Native

## üéØ Use Case : Microservices sur Azure Kubernetes

Entreprise qui migre 20 microservices vers Azure. Pipeline complet : Build ‚Üí Test ‚Üí Push ACR ‚Üí Deploy AKS.

## Pipeline YAML

```yaml
trigger:
  branches:
    include:
      - main

stages:
- stage: Build
  jobs:
  - job: Build
    steps:
    - task: Docker@2
      displayName: Build and push image
      inputs:
        command: buildAndPush
        repository: $(imageRepository)
```

## ROI

- **D√©ploiements** : 30 min ‚Üí 5 min
- **Rollback** : 1 commande
- **Co√ªts** : Pay-per-use, r√©duction 35%$BODY$,
    'Pipeline Azure DevOps complet avec AKS et ACR. Build, test, push, deploy automatis√©s. Microservices cloud-native sur Kubernetes manag√©.',
    '/images/tutorials/cloud-azure.svg',
    'Cloud',
    ARRAY['Azure', 'Cloud', 'AKS', 'DevOps', 'Kubernetes', 'CI/CD'],
    'published',
    NOW() - INTERVAL '85 days',
    132,
    25,
    'Azure DevOps + AKS : Pipeline CI/CD Cloud-Native Complet',
    'Pipeline Azure DevOps avec AKS et ACR. Build, test, deploy microservices. Kubernetes manag√© cloud-native.',
    ARRAY['azure', 'cloud', 'aks', 'devops', 'kubernetes', 'cicd']
);

-- CLOUD 3: GCP Cloud Run
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'GCP Cloud Run : Serverless Containers Auto-Scalant',
    'gcp-cloud-run-serverless',
    $BODY$# GCP Cloud Run : Serverless Container Platform

## üéØ Use Case : API Serverless qui Scale √† 0

API REST avec trafic variable. 0 requ√™tes la nuit ‚Üí 10K requ√™tes/sec en journ√©e. Cloud Run scale automatiquement et co√ªte 0‚Ç¨ quand inutilis√©.

## D√©ploiement Simple

```bash
gcloud run deploy myapi \
  --source . \
  --platform managed \
  --region europe-west1 \
  --allow-unauthenticated \
  --min-instances 0 \
  --max-instances 100
```

## ROI

- **Co√ªts** : 0‚Ç¨ quand inutilis√©
- **Scaling** : 0 √† 1000 instances en secondes
- **Maintenance** : 0 (manag√©)$BODY$,
    'D√©ployez des containers serverless avec GCP Cloud Run. Auto-scaling de 0 √† 1000 instances. Pay-per-use, co√ªts optimis√©s. Production-ready.',
    '/images/tutorials/cloud-gcp.svg',
    'Cloud',
    ARRAY['GCP', 'Cloud', 'Serverless', 'Cloud Run', 'Containers', 'Auto-Scaling'],
    'published',
    NOW() - INTERVAL '80 days',
    118,
    20,
    'GCP Cloud Run : Serverless Containers Auto-Scalant',
    'Cloud Run serverless pour containers. Scale de 0 √† 1000 instances. Pay-per-use, co√ªts optimis√©s.',
    ARRAY['gcp', 'cloud', 'serverless', 'cloud run', 'containers', 'autoscaling']
);

-- CLOUD 4: Multi-Cloud Terraform
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Multi-Cloud : D√©ployer sur AWS, Azure et GCP avec Terraform',
    'multi-cloud-terraform-aws-azure-gcp',
    $BODY$# Multi-Cloud avec Terraform

## üéØ Use Case : √âviter le Vendor Lock-In

Entreprise qui veut r√©partir workloads sur 3 clouds : AWS (compute), Azure (DB), GCP (ML). Terraform unifie tout.

## Configuration Multi-Provider

```hcl
terraform {
  required_providers {
    aws = { source = "hashicorp/aws", version = "~> 5.0" }
    azurerm = { source = "hashicorp/azurerm", version = "~> 3.0" }
    google = { source = "hashicorp/google", version = "~> 5.0" }
  }
}
```

## ROI

- **Flexibilit√©** : Meilleur service de chaque cloud
- **R√©silience** : Pas de single point of failure
- **Co√ªts** : Optimisation par workload$BODY$,
    'Infrastructure multi-cloud avec Terraform. AWS, Azure, GCP en 1 codebase. √âvitez le vendor lock-in. R√©silience et optimisation des co√ªts.',
    '/images/tutorials/cloud-multicloud.svg',
    'Cloud',
    ARRAY['Multi-Cloud', 'Terraform', 'AWS', 'Azure', 'GCP', 'IaC'],
    'published',
    NOW() - INTERVAL '75 days',
    156,
    22,
    'Multi-Cloud Terraform : AWS + Azure + GCP Unifi√©',
    'Infrastructure multi-cloud avec Terraform. AWS, Azure, GCP. Vendor lock-in √©vit√©, r√©silience maximale.',
    ARRAY['multi-cloud', 'terraform', 'aws', 'azure', 'gcp', 'iac']
);

-- ========================================
-- CAT√âGORIE : DEVOPS (4 tutoriels Docker)
-- ========================================

-- DOCKER 1: Multi-Stage Builds
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Docker Multi-Stage Builds : R√©duire vos Images de 1GB √† 50MB',
    'docker-multi-stage-builds-optimization',
    $BODY$# Docker Multi-Stage Builds

## üéØ Use Case : Image Node.js de 1.2GB ‚Üí 85MB

Application Node.js. Image initiale : 1.2GB. Apr√®s multi-stage : 85MB. Temps de d√©ploiement : -90%.

## Apr√®s : Multi-Stage (85MB)

```dockerfile
# Stage 1: Build
FROM node:20-alpine AS builder
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
RUN npm run build

# Stage 2: Production
FROM node:20-alpine
WORKDIR /app
COPY --from=builder /app/dist ./dist
COPY --from=builder /app/node_modules ./node_modules
USER node
CMD ["node", "dist/index.js"]
```

## ROI

- Taille : 1.2GB ‚Üí 85MB (-93%)
- Push DockerHub : 5 min ‚Üí 15 sec
- D√©ploiement K8s : 2 min ‚Üí 10 sec$BODY$,
    'Optimisez vos images Docker avec multi-stage builds. R√©duisez de 1GB √† 50MB. D√©ploiements 10x plus rapides. Production-ready.',
    '/images/tutorials/docker-multistage.svg',
    'DevOps',
    ARRAY['Docker', 'Multi-Stage', 'Optimization', 'DevOps', 'Performance'],
    'published',
    NOW() - INTERVAL '70 days',
    189,
    18,
    'Docker Multi-Stage : R√©duire Images de 1GB √† 50MB',
    'Multi-stage builds Docker. Images 93% plus petites. D√©ploiements ultra-rapides. Distroless images.',
    ARRAY['docker', 'multi-stage', 'optimization', 'performance']
);

-- DOCKER 2: Docker Compose
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Docker Compose : Stack Microservices Compl√®te en Local',
    'docker-compose-microservices-local',
    $BODY$# Docker Compose : Orchestration Locale

## üéØ Use Case : 10 Services en 1 Commande

Environnement local : API, DB, Redis, RabbitMQ, frontend. `docker compose up` = tout d√©marre en 30 secondes.

## Docker Compose complet

```yaml
version: '3.8'

services:
  api:
    build: ./api
    ports:
      - "3000:3000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/myapp
    depends_on:
      - db
      - redis

  db:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: myapp
      POSTGRES_PASSWORD: password
    volumes:
      - db-data:/var/lib/postgresql/data

volumes:
  db-data:
```

## ROI

- Onboarding : 5 min vs 2 jours
- Environnement identique pour toute l'√©quipe$BODY$,
    'Orchestrez vos microservices localement avec Docker Compose. Stack compl√®te en 1 commande. Onboarding devs en 5 minutes.',
    '/images/tutorials/docker-compose.svg',
    'DevOps',
    ARRAY['Docker', 'Docker Compose', 'Microservices', 'Development'],
    'published',
    NOW() - INTERVAL '65 days',
    167,
    16,
    'Docker Compose : Stack Microservices Locale Compl√®te',
    'Docker Compose pour d√©veloppement local. Multi-conteneurs, healthchecks. Stack en 1 commande.',
    ARRAY['docker', 'docker compose', 'microservices', 'development']
);

-- DOCKER 3: Security
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'S√©curit√© Docker : Hardening et Scan de Vuln√©rabilit√©s',
    'docker-security-hardening-best-practices',
    $BODY$# Docker Security Best Practices

## üéØ Use Case : Passer un Audit PCI-DSS

Application bancaire. Exigences : conteneurs non-root, images scann√©es, secrets chiffr√©s.

## Dockerfile S√©curis√©

```dockerfile
FROM node:20-alpine AS builder
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

FROM gcr.io/distroless/nodejs20-debian11
WORKDIR /app
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/dist ./dist
USER nonroot:nonroot
CMD ["dist/index.js"]
```

## Scan avec Trivy

```bash
trivy image myapp:latest
```

## ROI

- Vuln√©rabilit√©s : D√©tect√©es avant production
- Audit : Conformit√© automatique$BODY$,
    'S√©curisez vos conteneurs Docker. Hardening, scan vuln√©rabilit√©s, distroless images. Conformit√© audit PCI-DSS.',
    '/images/tutorials/docker-security.svg',
    'DevOps',
    ARRAY['Docker', 'Security', 'DevSecOps', 'Hardening'],
    'published',
    NOW() - INTERVAL '60 days',
    201,
    19,
    'Docker Security : Hardening et Scan Vuln√©rabilit√©s',
    'S√©curisez Docker. Scan vuln√©rabilit√©s, hardening, distroless. Conformit√© audit.',
    ARRAY['docker', 'security', 'devsecops', 'hardening']
);

-- DOCKER 4: Harbor Registry
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Harbor : Registry Docker Priv√© avec Scan Automatique',
    'docker-harbor-private-registry-security',
    $BODY$# Harbor : Private Docker Registry

## üéØ Use Case : Registry Priv√© Entreprise

50 images Docker priv√©es. Harbor = registry + scan vuln√©rabilit√©s + replication.

## Installation Harbor

```bash
wget https://github.com/goharbor/harbor/releases/download/v2.9.0/harbor-online-installer-v2.9.0.tgz
tar xzvf harbor-online-installer-v2.9.0.tgz
cd harbor
./install.sh
```

## Scan Automatique

Harbor scan automatiquement avec Trivy :
- CVE d√©tect√©es
- Secrets hardcod√©s
- Mauvaises configurations

## ROI

- Images scann√©es automatiquement
- Blocage images vuln√©rables
- Conformit√© s√©curit√©$BODY$,
    'Registry Docker priv√© avec Harbor. Scan automatique vuln√©rabilit√©s avec Trivy. Policies de s√©curit√©. Entreprise-ready.',
    '/images/tutorials/docker-harbor.svg',
    'DevOps',
    ARRAY['Docker', 'Harbor', 'Registry', 'Security', 'Trivy'],
    'published',
    NOW() - INTERVAL '55 days',
    178,
    21,
    'Harbor Registry Docker : Scan Vuln√©rabilit√©s Automatique',
    'Registry Docker priv√© Harbor. Scan Trivy, policies s√©curit√©. Conformit√© entreprise.',
    ARRAY['docker', 'harbor', 'registry', 'security', 'trivy']
);

-- ========================================
-- CAT√âGORIE : KUBERNETES (4 tutoriels)
-- ========================================

-- K8S 1: Production Cluster
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Kubernetes : Cluster Production-Ready avec Kubeadm',
    'kubernetes-production-cluster-setup',
    $BODY$# Kubernetes Production Cluster

## üéØ Use Case : E-commerce 24/7 Haute Disponibilit√©

Site e-commerce avec 100K visiteurs/jour. Kubernetes assure : haute disponibilit√©, auto-scaling, rolling updates sans downtime.

## Architecture

```
3 Master Nodes (HA) + 5 Worker Nodes
‚îú‚îÄ‚îÄ LoadBalancer (MetalLB)
‚îú‚îÄ‚îÄ Ingress (Nginx)
‚îú‚îÄ‚îÄ Storage (Longhorn)
‚îî‚îÄ‚îÄ Monitoring (Prometheus)
```

## Installation Kubeadm

```bash
# Sur chaque n≈ìud
sudo kubeadm init --control-plane-endpoint="lb.example.com:6443" \
  --upload-certs \
  --pod-network-cidr=10.244.0.0/16

# R√©seau CNI (Calico)
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
```

## D√©ploiement Application

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ecommerce-api
spec:
  replicas: 5
  template:
    spec:
      containers:
      - name: api
        image: myapp:v1.0
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
```

## ROI

- **Uptime** : 99.99% (4 pannes/an max)
- **Scaling** : 5 ‚Üí 50 pods automatiquement
- **Zero Downtime** : Rolling updates sans interruption$BODY$,
    'Cluster Kubernetes production-ready avec kubeadm. Haute disponibilit√©, auto-scaling, monitoring int√©gr√©. D√©ploiements sans downtime.',
    '/images/tutorials/k8s-cluster.svg',
    'Kubernetes',
    ARRAY['Kubernetes', 'K8s', 'Production', 'High Availability', 'Cluster'],
    'published',
    NOW() - INTERVAL '50 days',
    234,
    28,
    'Kubernetes Production Cluster : Setup Complet Haute Disponibilit√©',
    'Cluster Kubernetes HA avec kubeadm. 3 masters, auto-scaling, monitoring. Production-ready.',
    ARRAY['kubernetes', 'k8s', 'production', 'high availability', 'cluster']
);

-- K8S 2: Helm Charts
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Helm : Package Manager pour Kubernetes Applications',
    'helm-kubernetes-package-manager',
    $BODY$# Helm : Package Manager Kubernetes

## üéØ Use Case : D√©ployer 20 Microservices en 5 Minutes

Startup avec 20 microservices. Sans Helm : 200 fichiers YAML √† maintenir. Avec Helm : 1 chart, 20 d√©ploiements.

## Structure Chart Helm

```
mychart/
‚îú‚îÄ‚îÄ Chart.yaml
‚îú‚îÄ‚îÄ values.yaml
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îî‚îÄ‚îÄ ingress.yaml
```

## D√©ploiement Simple

```bash
# Installer un chart
helm install myapp ./mychart

# Upgrade
helm upgrade myapp ./mychart --set image.tag=v2.0

# Rollback instantan√©
helm rollback myapp 1
```

## Values.yaml

```yaml
replicaCount: 3
image:
  repository: myapp
  tag: "1.0"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 80

resources:
  limits:
    cpu: 500m
    memory: 512Mi
```

## ROI

- **D√©ploiements** : 2h ‚Üí 5 min
- **Rollback** : 1 commande vs 30 min manuel
- **R√©utilisabilit√©** : 1 chart = infinite d√©ploiements$BODY$,
    'Helm package manager pour Kubernetes. Simplifiez d√©ploiements avec charts r√©utilisables. Rollback en 1 commande. Production-ready.',
    '/images/tutorials/helm-k8s.svg',
    'Kubernetes',
    ARRAY['Helm', 'Kubernetes', 'Package Manager', 'Charts', 'DevOps'],
    'published',
    NOW() - INTERVAL '45 days',
    198,
    24,
    'Helm Kubernetes : Package Manager pour Apps Cloud-Native',
    'Helm charts pour Kubernetes. D√©ploiements simplifi√©s, rollback instantan√©. Templates r√©utilisables.',
    ARRAY['helm', 'kubernetes', 'package manager', 'charts', 'devops']
);

-- K8S 3: Monitoring Stack
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Monitoring Kubernetes : Stack Prometheus + Grafana Compl√®te',
    'kubernetes-monitoring-prometheus-grafana',
    $BODY$# Monitoring Kubernetes avec Prometheus

## üéØ Use Case : Observer 100 Pods en Temps R√©el

Cluster avec 100 pods. Stack monitoring : Prometheus (m√©triques) + Grafana (dashboards) + AlertManager (alertes).

## Installation kube-prometheus-stack

```bash
# Ajouter repo Helm
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Installer stack compl√®te
helm install monitoring prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace
```

## M√©triques Collect√©es

- CPU/Memory par pod
- Taux de requ√™tes HTTP
- Latence P50/P95/P99
- Erreurs 5xx
- Disk I/O

## Dashboards Grafana

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard
data:
  dashboard.json: |
    {
      "panels": [
        {
          "title": "Pod CPU Usage",
          "targets": [
            {
              "expr": "rate(container_cpu_usage_seconds_total[5m])"
            }
          ]
        }
      ]
    }
```

## ROI

- **D√©tection incidents** : 30 min ‚Üí 30 sec
- **Root cause analysis** : 2h ‚Üí 10 min
- **Co√ªts cloud** : -25% via optimisation ressources$BODY$,
    'Stack monitoring Kubernetes compl√®te. Prometheus + Grafana + AlertManager. M√©triques temps r√©el, dashboards, alertes automatiques.',
    '/images/tutorials/k8s-monitoring.svg',
    'Kubernetes',
    ARRAY['Kubernetes', 'Monitoring', 'Prometheus', 'Grafana', 'Observability'],
    'published',
    NOW() - INTERVAL '40 days',
    212,
    26,
    'Monitoring Kubernetes : Prometheus + Grafana Stack Compl√®te',
    'Stack monitoring K8s. Prometheus, Grafana, AlertManager. Dashboards, m√©triques, alertes temps r√©el.',
    ARRAY['kubernetes', 'monitoring', 'prometheus', 'grafana', 'observability']
);

-- K8S 4: Service Mesh Istio
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Istio : Service Mesh pour Microservices Kubernetes',
    'istio-service-mesh-kubernetes',
    $BODY$# Istio Service Mesh

## üéØ Use Case : S√©curiser 50 Microservices

Architecture microservices avec 50 services. Istio g√®re : mTLS automatique, traffic management, observabilit√©, circuit breakers.

## Installation Istio

```bash
# T√©l√©charger Istio
curl -L https://istio.io/downloadIstio | sh -
cd istio-1.20.0
export PATH=$PWD/bin:$PATH

# Installer
istioctl install --set profile=production -y

# Activer injection sidecar
kubectl label namespace default istio-injection=enabled
```

## Traffic Management

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
  - reviews
  http:
  - match:
    - headers:
        user:
          exact: canary
    route:
    - destination:
        host: reviews
        subset: v2
      weight: 100
  - route:
    - destination:
        host: reviews
        subset: v1
      weight: 90
    - destination:
        host: reviews
        subset: v2
      weight: 10
```

## ROI

- **S√©curit√©** : mTLS automatique entre tous les services
- **Canary Deployments** : 0 downtime
- **Observabilit√©** : Tracing distribu√© Jaeger$BODY$,
    'Service mesh Istio pour Kubernetes. mTLS automatique, traffic management, observabilit√© avanc√©e. S√©curisez vos microservices.',
    '/images/tutorials/istio-mesh.svg',
    'Kubernetes',
    ARRAY['Istio', 'Service Mesh', 'Kubernetes', 'Microservices', 'mTLS'],
    'published',
    NOW() - INTERVAL '35 days',
    187,
    30,
    'Istio Service Mesh : S√©curit√© et Observabilit√© Microservices',
    'Istio pour Kubernetes. mTLS, traffic management, tracing. Service mesh production-ready.',
    ARRAY['istio', 'service mesh', 'kubernetes', 'microservices', 'mtls']
);

-- ========================================
-- CAT√âGORIE : CI/CD (4 tutoriels)
-- ========================================

-- CI/CD 1: GitHub Actions
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'GitHub Actions : Pipeline CI/CD Complet du Test au D√©ploiement',
    'github-actions-cicd-pipeline-complete',
    $BODY$# GitHub Actions : Pipeline CI/CD

## üéØ Use Case : Automatiser tout le Cycle de Vie

Application web. √Ä chaque push : tests ‚Üí build ‚Üí scan s√©cu ‚Üí deploy. Tout automatique, z√©ro intervention manuelle.

## Workflow Complet

```yaml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      - run: npm ci
      - run: npm test
      - run: npm run test:coverage

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Build Docker image
        run: docker build -t myapp:${{ github.sha }} .
      - name: Push to Registry
        run: |
          echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
          docker push myapp:${{ github.sha }}

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Deploy to Kubernetes
        run: |
          kubectl set image deployment/myapp myapp=myapp:${{ github.sha }}
          kubectl rollout status deployment/myapp
```

## ROI

- **D√©ploiements** : 1h ‚Üí 5 min automatique
- **Bugs d√©tect√©s** : +80% via tests auto
- **Time-to-market** : -50%$BODY$,
    'Pipeline CI/CD complet avec GitHub Actions. Tests auto, build, scan s√©curit√©, d√©ploiement. Z√©ro intervention manuelle.',
    '/images/tutorials/github-actions.svg',
    'CI/CD',
    ARRAY['GitHub Actions', 'CI/CD', 'DevOps', 'Automation', 'Pipeline'],
    'published',
    NOW() - INTERVAL '30 days',
    267,
    22,
    'GitHub Actions : Pipeline CI/CD Automatique Complet',
    'GitHub Actions CI/CD. Tests, build, deploy automatiques. Pipeline production-ready.',
    ARRAY['github actions', 'cicd', 'devops', 'automation', 'pipeline']
);

-- CI/CD 2: GitLab CI
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'GitLab CI : Pipeline Multi-Environnements avec Auto DevOps',
    'gitlab-ci-pipeline-multi-environments',
    $BODY$# GitLab CI : Pipeline Multi-Env

## üéØ Use Case : Dev ‚Üí Staging ‚Üí Production Automatique

3 environnements isol√©s. Pipeline : push dev ‚Üí tests ‚Üí deploy staging ‚Üí tests e2e ‚Üí deploy prod (approbation manuelle).

## .gitlab-ci.yml

```yaml
stages:
  - test
  - build
  - deploy-staging
  - deploy-production

variables:
  DOCKER_IMAGE: registry.gitlab.com/$CI_PROJECT_PATH

test:
  stage: test
  script:
    - npm ci
    - npm run test
    - npm run lint
  coverage: '/All files[^|]*\|[^|]*\s+([\d\.]+)/'

build:
  stage: build
  script:
    - docker build -t $DOCKER_IMAGE:$CI_COMMIT_SHA .
    - docker push $DOCKER_IMAGE:$CI_COMMIT_SHA

deploy:staging:
  stage: deploy-staging
  script:
    - kubectl config use-context staging
    - helm upgrade --install myapp ./helm --set image.tag=$CI_COMMIT_SHA
  environment:
    name: staging
    url: https://staging.example.com

deploy:production:
  stage: deploy-production
  script:
    - kubectl config use-context production
    - helm upgrade --install myapp ./helm --set image.tag=$CI_COMMIT_SHA
  environment:
    name: production
    url: https://example.com
  when: manual
  only:
    - main
```

## ROI

- **Environnements isol√©s** : 0 conflit
- **Approbations** : Contr√¥le humain sur production
- **Tra√ßabilit√©** : Audit complet des d√©ploiements$BODY$,
    'Pipeline GitLab CI multi-environnements. Dev, staging, production automatis√©s. Approbations manuelles, tra√ßabilit√© compl√®te.',
    '/images/tutorials/gitlab-ci.svg',
    'CI/CD',
    ARRAY['GitLab CI', 'CI/CD', 'Multi-Environment', 'DevOps', 'Pipeline'],
    'published',
    NOW() - INTERVAL '25 days',
    243,
    24,
    'GitLab CI : Pipeline Multi-Environnements Production-Ready',
    'GitLab CI multi-env. Dev, staging, prod. Approbations, rollback, tra√ßabilit√©.',
    ARRAY['gitlab ci', 'cicd', 'multi-environment', 'devops', 'pipeline']
);

-- CI/CD 3: Jenkins Pipeline
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Jenkins : Pipeline as Code avec Groovy et Docker',
    'jenkins-pipeline-as-code-groovy-docker',
    $BODY$# Jenkins Pipeline as Code

## üéØ Use Case : Legacy CI/CD Modernis√©

Migration CI/CD Jenkins classique ‚Üí Pipeline as Code. 50 jobs manuels ‚Üí 1 Jenkinsfile versionn√© Git.

## Jenkinsfile

```groovy
pipeline {
    agent {
        docker {
            image 'node:20-alpine'
        }
    }

    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }

        stage('Install') {
            steps {
                sh 'npm ci'
            }
        }

        stage('Test') {
            parallel {
                stage('Unit Tests') {
                    steps {
                        sh 'npm run test:unit'
                    }
                }
                stage('Integration Tests') {
                    steps {
                        sh 'npm run test:integration'
                    }
                }
                stage('Lint') {
                    steps {
                        sh 'npm run lint'
                    }
                }
            }
        }

        stage('Build') {
            steps {
                sh 'npm run build'
            }
        }

        stage('Docker Build & Push') {
            steps {
                script {
                    docker.build("myapp:${env.BUILD_ID}").push()
                }
            }
        }

        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                sh 'kubectl set image deployment/myapp myapp=myapp:${BUILD_ID}'
            }
        }
    }

    post {
        success {
            slackSend color: 'good', message: "Build ${env.BUILD_ID} r√©ussi!"
        }
        failure {
            slackSend color: 'danger', message: "Build ${env.BUILD_ID} √©chou√©!"
        }
    }
}
```

## ROI

- **Versionning** : Pipeline dans Git
- **Tests parall√®les** : Temps divis√© par 3
- **Notifications** : Slack/Email automatiques$BODY$,
    'Jenkins Pipeline as Code avec Groovy. Jenkinsfile versionn√©, tests parall√®les, notifications automatiques. Legacy modernis√©.',
    '/images/tutorials/jenkins-pipeline.svg',
    'CI/CD',
    ARRAY['Jenkins', 'Pipeline', 'Groovy', 'CI/CD', 'DevOps'],
    'published',
    NOW() - INTERVAL '20 days',
    189,
    26,
    'Jenkins Pipeline as Code : Jenkinsfile et Docker',
    'Jenkins Pipeline moderne. Jenkinsfile versionn√©, tests parall√®les, Docker agents.',
    ARRAY['jenkins', 'pipeline', 'groovy', 'cicd', 'devops']
);

-- CI/CD 4: ArgoCD GitOps
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'ArgoCD : GitOps pour Kubernetes - CD D√©claratif',
    'argocd-gitops-kubernetes-declarative',
    $BODY$# ArgoCD : GitOps Kubernetes

## üéØ Use Case : Git comme Source de V√©rit√©

Infrastructure Kubernetes. Git = source unique de v√©rit√©. ArgoCD synchronise automatiquement cluster avec Git repo.

## Installation ArgoCD

```bash
# Installer ArgoCD
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# Acc√©der UI
kubectl port-forward svc/argocd-server -n argocd 8080:443
```

## Application Manifest

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: myapp
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/myorg/myapp
    targetRevision: HEAD
    path: k8s/
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
```

## Workflow GitOps

1. Dev modifie YAML dans Git
2. Pull Request ‚Üí Review
3. Merge vers main
4. ArgoCD d√©tecte changement
5. Sync automatique vers cluster

## ROI

- **Audit** : Tout changement trac√© dans Git
- **Rollback** : Git revert = rollback instant
- **Disaster Recovery** : Cluster recr√©√© depuis Git$BODY$,
    'ArgoCD GitOps pour Kubernetes. Git comme source de v√©rit√©, sync automatique, rollback facile. CD d√©claratif production-ready.',
    '/images/tutorials/argocd-gitops.svg',
    'CI/CD',
    ARRAY['ArgoCD', 'GitOps', 'Kubernetes', 'CD', 'Declarative'],
    'published',
    NOW() - INTERVAL '15 days',
    278,
    28,
    'ArgoCD GitOps : Continuous Delivery D√©claratif Kubernetes',
    'ArgoCD pour GitOps K8s. Git source de v√©rit√©, sync auto, rollback instant.',
    ARRAY['argocd', 'gitops', 'kubernetes', 'cd', 'declarative']
);

-- ========================================
-- CAT√âGORIE : TERRAFORM (3 tutoriels)
-- ========================================

-- TERRAFORM 1: AWS Infrastructure
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Terraform AWS : Infrastructure as Code Production-Ready',
    'terraform-aws-infrastructure-as-code',
    $BODY$# Terraform AWS IaC

## üéØ Use Case : Cr√©er Infrastructure AWS en 5 Minutes

Infrastructure AWS compl√®te : VPC, subnets, EC2, RDS, S3. Sans Terraform : 2 jours. Avec Terraform : 5 min.

## Structure Projet

```
terraform/
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ outputs.tf
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ vpc/
‚îÇ   ‚îú‚îÄ‚îÄ compute/
‚îÇ   ‚îî‚îÄ‚îÄ database/
```

## VPC Module

```hcl
module "vpc" {
  source = "./modules/vpc"

  vpc_cidr = "10.0.0.0/16"
  azs      = ["eu-west-1a", "eu-west-1b", "eu-west-1c"]

  public_subnets  = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
  private_subnets = ["10.0.11.0/24", "10.0.12.0/24", "10.0.13.0/24"]

  enable_nat_gateway = true
  enable_vpn_gateway = false

  tags = {
    Environment = "production"
    Project     = "myapp"
  }
}
```

## D√©ploiement

```bash
terraform init
terraform plan
terraform apply -auto-approve
```

## ROI

- **Reproductibilit√©** : M√™me infra en 1 commande
- **Versioning** : Infrastructure dans Git
- **Destruction** : terraform destroy = cleanup complet$BODY$,
    'Terraform pour AWS. Infrastructure as Code production-ready. VPC, EC2, RDS, S3. Reproductible, versionn√©, automatis√©.',
    '/images/tutorials/terraform-aws.svg',
    'Terraform',
    ARRAY['Terraform', 'AWS', 'IaC', 'Infrastructure', 'Automation'],
    'published',
    NOW() - INTERVAL '12 days',
    298,
    25,
    'Terraform AWS : Infrastructure as Code Complete',
    'Terraform pour AWS. IaC production-ready. VPC, compute, database. Modules r√©utilisables.',
    ARRAY['terraform', 'aws', 'iac', 'infrastructure', 'automation']
);

-- TERRAFORM 2: Modules R√©utilisables
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Terraform Modules : Cr√©er des Composants R√©utilisables',
    'terraform-modules-reusable-components',
    $BODY$# Terraform Modules R√©utilisables

## üéØ Use Case : DRY Infrastructure Code

15 microservices avec infra similaire. Sans modules : copier-coller. Avec modules : 1 d√©finition, 15 instances.

## Module Structure

```
modules/microservice/
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ outputs.tf
‚îî‚îÄ‚îÄ README.md
```

## Module Microservice

```hcl
# modules/microservice/main.tf
resource "aws_ecs_service" "this" {
  name            = var.service_name
  cluster         = var.cluster_id
  task_definition = aws_ecs_task_definition.this.arn
  desired_count   = var.desired_count

  load_balancer {
    target_group_arn = aws_lb_target_group.this.arn
    container_name   = var.service_name
    container_port   = var.container_port
  }
}

resource "aws_lb_target_group" "this" {
  name     = "${var.service_name}-tg"
  port     = var.container_port
  protocol = "HTTP"
  vpc_id   = var.vpc_id

  health_check {
    path                = var.health_check_path
    healthy_threshold   = 2
    unhealthy_threshold = 10
  }
}
```

## Utilisation

```hcl
module "api" {
  source = "./modules/microservice"

  service_name      = "api"
  cluster_id        = aws_ecs_cluster.main.id
  vpc_id            = module.vpc.id
  desired_count     = 3
  container_port    = 3000
  health_check_path = "/health"
}

module "worker" {
  source = "./modules/microservice"

  service_name      = "worker"
  cluster_id        = aws_ecs_cluster.main.id
  vpc_id            = module.vpc.id
  desired_count     = 5
  container_port    = 3001
  health_check_path = "/ready"
}
```

## ROI

- **DRY** : 1 d√©finition, N instances
- **Maintenance** : Update module = update all
- **Standards** : Best practices enforced$BODY$,
    'Terraform modules r√©utilisables. DRY infrastructure, maintenance simplifi√©e, best practices enforced. Composants standardis√©s.',
    '/images/tutorials/terraform-modules.svg',
    'Terraform',
    ARRAY['Terraform', 'Modules', 'IaC', 'Reusability', 'Best Practices'],
    'published',
    NOW() - INTERVAL '10 days',
    176,
    22,
    'Terraform Modules : Composants R√©utilisables IaC',
    'Terraform modules. Infrastructure DRY, r√©utilisable, standardis√©e. Best practices.',
    ARRAY['terraform', 'modules', 'iac', 'reusability', 'best practices']
);

-- TERRAFORM 3: State Management
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Terraform State : Remote Backend et Locking S3/DynamoDB',
    'terraform-state-remote-backend-s3',
    $BODY$# Terraform State Management

## üéØ Use Case : Travail en √âquipe sans Conflits

√âquipe de 10 DevOps. State local = conflits. Remote state S3 + locking DynamoDB = collaboration fluide.

## Backend Configuration

```hcl
terraform {
  backend "s3" {
    bucket         = "mycompany-terraform-state"
    key            = "production/terraform.tfstate"
    region         = "eu-west-1"
    encrypt        = true
    dynamodb_table = "terraform-locks"
  }
}
```

## Setup Backend

```bash
# Cr√©er bucket S3
aws s3 mb s3://mycompany-terraform-state
aws s3api put-bucket-versioning \
  --bucket mycompany-terraform-state \
  --versioning-configuration Status=Enabled

# Cr√©er table DynamoDB pour locking
aws dynamodb create-table \
  --table-name terraform-locks \
  --attribute-definitions AttributeName=LockID,AttributeType=S \
  --key-schema AttributeName=LockID,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST
```

## Workspaces

```bash
# Cr√©er workspaces pour environnements
terraform workspace new dev
terraform workspace new staging
terraform workspace new production

# Utiliser workspace
terraform workspace select production
terraform apply
```

## ROI

- **Collaboration** : 10 DevOps sans conflits
- **Versioning** : State versionn√© S3
- **S√©curit√©** : State chiffr√©, locking automatique$BODY$,
    'Terraform state remote avec S3 et DynamoDB. Collaboration √©quipe, locking, versioning. State s√©curis√© et partag√©.',
    '/images/tutorials/terraform-state.svg',
    'Terraform',
    ARRAY['Terraform', 'State', 'S3', 'DynamoDB', 'Team Collaboration'],
    'published',
    NOW() - INTERVAL '8 days',
    203,
    20,
    'Terraform State : Remote Backend S3 et Locking',
    'Terraform remote state. S3 backend, DynamoDB locking. Collaboration √©quipe s√©curis√©e.',
    ARRAY['terraform', 'state', 's3', 'dynamodb', 'collaboration']
);

-- ========================================
-- CAT√âGORIE : ANSIBLE (3 tutoriels)
-- ========================================

-- ANSIBLE 1: Server Configuration
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Ansible : Automatiser la Configuration de 100 Serveurs',
    'ansible-server-configuration-automation',
    $BODY$# Ansible : Configuration Management

## üéØ Use Case : Configurer 100 Serveurs en 10 Minutes

Datacenter avec 100 serveurs Ubuntu. Manuellement : 2 semaines. Avec Ansible : 10 minutes.

## Inventory

```ini
[webservers]
web[01:50].example.com

[databases]
db[01:10].example.com

[loadbalancers]
lb[01:05].example.com

[all:vars]
ansible_user=ubuntu
ansible_ssh_private_key_file=~/.ssh/id_rsa
```

## Playbook Web Servers

```yaml
---
- name: Configure Web Servers
  hosts: webservers
  become: yes

  tasks:
    - name: Install packages
      apt:
        name:
          - nginx
          - nodejs
          - npm
        state: present
        update_cache: yes

    - name: Configure Nginx
      template:
        src: templates/nginx.conf.j2
        dest: /etc/nginx/sites-available/default
      notify: restart nginx

    - name: Ensure Nginx is running
      service:
        name: nginx
        state: started
        enabled: yes

  handlers:
    - name: restart nginx
      service:
        name: nginx
        state: restarted
```

## Ex√©cution

```bash
ansible-playbook -i inventory playbook.yml
```

## ROI

- **Vitesse** : 100 serveurs en 10 min
- **Idempotence** : R√©ex√©cution safe
- **Documentation** : Playbook = doc vivante$BODY$,
    'Ansible pour configuration management. Automatisez 100+ serveurs en minutes. Playbooks idempotents, documentation vivante.',
    '/images/tutorials/ansible-config.svg',
    'Ansible',
    ARRAY['Ansible', 'Automation', 'Configuration', 'DevOps', 'Infrastructure'],
    'published',
    NOW() - INTERVAL '6 days',
    198,
    24,
    'Ansible : Automatisation Configuration Serveurs √† l''√âchelle',
    'Ansible configuration management. 100+ serveurs automatis√©s. Playbooks idempotents.',
    ARRAY['ansible', 'automation', 'configuration', 'devops', 'infrastructure']
);

-- ANSIBLE 2: Roles et Galaxy
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Ansible Roles : R√©utiliser et Partager vos Playbooks',
    'ansible-roles-galaxy-reusable',
    $BODY$# Ansible Roles & Galaxy

## üéØ Use Case : Biblioth√®que de R√¥les R√©utilisables

10 projets avec configurations similaires. Cr√©er r√¥les r√©utilisables : DRY, maintenabilit√©, Ansible Galaxy.

## Structure Role

```
roles/webserver/
‚îú‚îÄ‚îÄ tasks/
‚îÇ   ‚îî‚îÄ‚îÄ main.yml
‚îú‚îÄ‚îÄ handlers/
‚îÇ   ‚îî‚îÄ‚îÄ main.yml
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ nginx.conf.j2
‚îú‚îÄ‚îÄ files/
‚îÇ   ‚îî‚îÄ‚îÄ index.html
‚îú‚îÄ‚îÄ vars/
‚îÇ   ‚îî‚îÄ‚îÄ main.yml
‚îú‚îÄ‚îÄ defaults/
‚îÇ   ‚îî‚îÄ‚îÄ main.yml
‚îî‚îÄ‚îÄ meta/
    ‚îî‚îÄ‚îÄ main.yml
```

## Role Webserver

```yaml
# roles/webserver/tasks/main.yml
---
- name: Install Nginx
  apt:
    name: nginx
    state: present

- name: Configure Nginx
  template:
    src: nginx.conf.j2
    dest: /etc/nginx/nginx.conf
  notify: restart nginx

- name: Start Nginx
  service:
    name: nginx
    state: started
    enabled: yes
```

## Utilisation

```yaml
---
- name: Setup Web Servers
  hosts: webservers
  roles:
    - common
    - security
    - webserver
    - monitoring
```

## Ansible Galaxy

```bash
# Rechercher r√¥les
ansible-galaxy search nginx

# Installer role
ansible-galaxy install geerlingguy.nginx

# Cr√©er role
ansible-galaxy init my-role
```

## ROI

- **R√©utilisabilit√©** : 1 role = N projets
- **Community** : 20K+ roles Ansible Galaxy
- **Standardisation** : Best practices partag√©es$BODY$,
    'Ansible roles et Galaxy. Cr√©ez composants r√©utilisables, partagez sur Galaxy. DRY automation, community-driven.',
    '/images/tutorials/ansible-roles.svg',
    'Ansible',
    ARRAY['Ansible', 'Roles', 'Galaxy', 'Reusability', 'Community'],
    'published',
    NOW() - INTERVAL '4 days',
    167,
    21,
    'Ansible Roles : Playbooks R√©utilisables et Galaxy',
    'Ansible roles r√©utilisables. Galaxy community, DRY automation. Best practices.',
    ARRAY['ansible', 'roles', 'galaxy', 'reusability', 'community']
);

-- ANSIBLE 3: Dynamic Inventory
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Ansible Dynamic Inventory : AWS EC2 et Cloud Discovery',
    'ansible-dynamic-inventory-aws-cloud',
    $BODY$# Ansible Dynamic Inventory

## üéØ Use Case : Auto-Discovery Instances Cloud

Infrastructure cloud dynamique. Instances cr√©√©es/supprim√©es fr√©quemment. Dynamic inventory = d√©couverte automatique.

## AWS EC2 Plugin

```yaml
# aws_ec2.yml
plugin: aws_ec2
regions:
  - eu-west-1
  - us-east-1

filters:
  instance-state-name: running

keyed_groups:
  - key: tags.Environment
    prefix: env
  - key: tags.Role
    prefix: role
  - key: placement.availability_zone
    prefix: az

compose:
  ansible_host: public_ip_address
```

## Utilisation

```bash
# Lister inventory
ansible-inventory -i aws_ec2.yml --list

# Ex√©cuter playbook
ansible-playbook -i aws_ec2.yml playbook.yml

# Cibler groupe sp√©cifique
ansible-playbook -i aws_ec2.yml playbook.yml --limit env_production
```

## Playbook avec Groupes Dynamiques

```yaml
---
- name: Configure Web Servers
  hosts: role_webserver
  tasks:
    - name: Update packages
      apt:
        upgrade: safe

- name: Configure Databases
  hosts: role_database
  tasks:
    - name: Backup databases
      shell: pg_dump mydb > backup.sql
```

## ROI

- **Auto-Discovery** : Nouvelles instances auto-d√©tect√©es
- **No Maintenance** : Inventory mis √† jour automatiquement
- **Cloud-Native** : Support AWS, Azure, GCP$BODY$,
    'Ansible dynamic inventory pour cloud. Auto-discovery instances AWS EC2, Azure, GCP. Pas de maintenance inventory manuelle.',
    '/images/tutorials/ansible-dynamic.svg',
    'Ansible',
    ARRAY['Ansible', 'Dynamic Inventory', 'AWS', 'Cloud', 'Automation'],
    'published',
    NOW() - INTERVAL '2 days',
    189,
    19,
    'Ansible Dynamic Inventory : Auto-Discovery Cloud',
    'Ansible dynamic inventory. AWS EC2, Azure, GCP. Auto-discovery instances cloud.',
    ARRAY['ansible', 'dynamic inventory', 'aws', 'cloud', 'automation']
);

-- ========================================
-- CAT√âGORIE : MONITORING (3 tutoriels)
-- ========================================

-- MONITORING 1: Prometheus + Grafana
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Prometheus + Grafana : Stack Monitoring Production Compl√®te',
    'prometheus-grafana-monitoring-stack',
    $BODY$# Prometheus + Grafana Stack

## üéØ Use Case : Monitoring 200 Services en Production

Infrastructure avec 200 microservices. Stack monitoring : Prometheus (m√©triques) + Grafana (dashboards) + AlertManager (alertes).

## Docker Compose Stack

```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana:latest
    volumes:
      - grafana-data:/var/lib/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin

  alertmanager:
    image: prom/alertmanager:latest
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
    ports:
      - "9093:9093"

volumes:
  prometheus-data:
  grafana-data:
```

## Configuration Prometheus

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']

  - job_name: 'api'
    static_configs:
      - targets: ['api:3000']
```

## Dashboard Grafana

```json
{
  "dashboard": {
    "title": "System Monitoring",
    "panels": [
      {
        "title": "CPU Usage",
        "targets": [
          {
            "expr": "100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)"
          }
        ]
      }
    ]
  }
}
```

## ROI

- **Visibilit√©** : M√©triques temps r√©el 200 services
- **Alertes** : Incidents d√©tect√©s en <30 sec
- **Debugging** : Root cause analysis facilit√©$BODY$,
    'Stack monitoring Prometheus + Grafana compl√®te. M√©triques temps r√©el, dashboards, alertes. Production-ready pour microservices.',
    '/images/tutorials/prometheus-grafana.svg',
    'Monitoring',
    ARRAY['Prometheus', 'Grafana', 'Monitoring', 'Observability', 'Metrics'],
    'published',
    NOW() - INTERVAL '28 days',
    312,
    27,
    'Prometheus + Grafana : Stack Monitoring Production',
    'Prometheus Grafana monitoring. M√©triques, dashboards, alertes. Stack production-ready.',
    ARRAY['prometheus', 'grafana', 'monitoring', 'observability', 'metrics']
);

-- MONITORING 2: ELK Stack
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'ELK Stack : Centralisez vos Logs avec Elasticsearch',
    'elk-stack-centralized-logging',
    $BODY$# ELK Stack : Centralized Logging

## üéØ Use Case : Logs de 100 Serveurs Centralis√©s

100 serveurs = 100 fichiers logs dispers√©s. ELK Stack : Elasticsearch (stockage) + Logstash (collecte) + Kibana (visualisation).

## Docker Compose ELK

```yaml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"

  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    ports:
      - "5000:5000"

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
```

## Logstash Pipeline

```
input {
  beats {
    port => 5044
  }
  tcp {
    port => 5000
    codec => json
  }
}

filter {
  if [type] == "nginx" {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
  }

  date {
    match => [ "timestamp", "ISO8601" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
}
```

## Filebeat Configuration

```yaml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/nginx/*.log
  fields:
    type: nginx

output.logstash:
  hosts: ["logstash:5044"]
```

## ROI

- **Centralisation** : 1 interface, 100 sources
- **Recherche** : Full-text search milliseconde
- **Alertes** : Patterns anormaux d√©tect√©s auto$BODY$,
    'ELK Stack pour logs centralis√©s. Elasticsearch, Logstash, Kibana. Full-text search, visualisations, alertes. Production-ready.',
    '/images/tutorials/elk-stack.svg',
    'Monitoring',
    ARRAY['ELK', 'Elasticsearch', 'Logstash', 'Kibana', 'Logging'],
    'published',
    NOW() - INTERVAL '22 days',
    267,
    29,
    'ELK Stack : Logs Centralis√©s avec Elasticsearch',
    'ELK Stack logging. Elasticsearch Logstash Kibana. Logs centralis√©s, recherche, alertes.',
    ARRAY['elk', 'elasticsearch', 'logstash', 'kibana', 'logging']
);

-- MONITORING 3: Distributed Tracing
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Jaeger : Distributed Tracing pour Microservices',
    'jaeger-distributed-tracing-microservices',
    $BODY$# Jaeger Distributed Tracing

## üéØ Use Case : D√©bugger Latence dans 20 Microservices

Requ√™te traverse 20 microservices. Latence : 5 sec. O√π est le probl√®me ? Jaeger tracing identifie le coupable.

## Architecture

```
Client ‚Üí API Gateway ‚Üí Service A ‚Üí Service B ‚Üí Service C ‚Üí DB
                    ‚Üì            ‚Üì            ‚Üì
                  Jaeger       Jaeger       Jaeger
```

## Installation Jaeger

```bash
# Docker
docker run -d --name jaeger \
  -p 16686:16686 \
  -p 14268:14268 \
  jaegertracing/all-in-one:latest
```

## Instrumentation Node.js

```javascript
const { initTracer } = require('jaeger-client');

const config = {
  serviceName: 'my-service',
  sampler: {
    type: 'const',
    param: 1,
  },
  reporter: {
    logSpans: true,
    agentHost: 'localhost',
    agentPort: 6831,
  },
};

const tracer = initTracer(config);

// Span cr√©ation
const span = tracer.startSpan('http_request');
span.setTag('http.method', 'GET');
span.setTag('http.url', '/api/users');

// Op√©ration
await doWork();

span.finish();
```

## Analyse Trace

```
Request ID: abc123
Total: 5200ms

‚îú‚îÄ API Gateway (50ms)
‚îú‚îÄ Auth Service (200ms)
‚îú‚îÄ User Service (100ms)
‚îú‚îÄ Order Service (4800ms) ‚Üê PROBL√àME ICI
‚îÇ  ‚îú‚îÄ Database Query (4750ms) ‚Üê REQU√äTE LENTE
‚îÇ  ‚îî‚îÄ Cache Check (50ms)
‚îî‚îÄ Response (50ms)
```

## ROI

- **Debugging** : Probl√®me identifi√© en minutes
- **Optimisation** : Bottlenecks visualis√©s
- **SLA** : Respect garantis via monitoring latence$BODY$,
    'Jaeger distributed tracing pour microservices. Identifiez latences, d√©bugguez requ√™tes complexes. OpenTelemetry compatible.',
    '/images/tutorials/jaeger-tracing.svg',
    'Monitoring',
    ARRAY['Jaeger', 'Tracing', 'Distributed', 'Microservices', 'Observability'],
    'published',
    NOW() - INTERVAL '18 days',
    198,
    25,
    'Jaeger : Distributed Tracing pour Microservices',
    'Jaeger tracing distribu√©. D√©buggage microservices, latence, bottlenecks. OpenTelemetry.',
    ARRAY['jaeger', 'tracing', 'distributed', 'microservices', 'observability']
);

-- ========================================
-- CAT√âGORIE : AUTOMATION (3 tutoriels)
-- ========================================

-- AUTOMATION 1: Python Scripts
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Python : 10 Scripts DevOps pour Automatiser le Quotidien',
    'python-devops-automation-scripts',
    $BODY$# Python DevOps Automation

## üéØ Use Case : Automatiser T√¢ches R√©p√©titives

T√¢ches manuelles quotidiennes : backups, cleanups, monitoring. Python scripts = automation compl√®te.

## Script 1: Backup Automatique

```python
import boto3
from datetime import datetime

def backup_to_s3(file_path, bucket_name):
    s3 = boto3.client('s3')

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    key = f'backups/{timestamp}_{file_path}'

    s3.upload_file(file_path, bucket_name, key)
    print(f'‚úÖ Backup uploaded: {key}')

# Usage
backup_to_s3('/var/lib/postgresql/backup.sql', 'my-backups')
```

## Script 2: Cleanup Docker

```python
import docker

def cleanup_old_images():
    client = docker.from_env()

    # Supprimer images danglings
    client.images.prune(filters={'dangling': True})

    # Supprimer conteneurs stopped
    client.containers.prune()

    # Supprimer volumes unused
    client.volumes.prune()

    print('‚úÖ Docker cleanup completed')

cleanup_old_images()
```

## Script 3: Health Check

```python
import requests
from slack_sdk import WebClient

SERVICES = [
    'https://api.example.com/health',
    'https://app.example.com/health',
]

slack = WebClient(token='xoxb-token')

for url in SERVICES:
    try:
        r = requests.get(url, timeout=5)
        if r.status_code != 200:
            slack.chat_postMessage(
                channel='#alerts',
                text=f'üö® {url} is DOWN!'
            )
    except Exception as e:
        slack.chat_postMessage(
            channel='#alerts',
            text=f'üö® {url} error: {e}'
        )
```

## Cron Automation

```bash
# Crontab
0 2 * * * python3 /scripts/backup.py
0 3 * * * python3 /scripts/cleanup.py
*/5 * * * * python3 /scripts/healthcheck.py
```

## ROI

- **Temps gagn√©** : 2h/jour ‚Üí 0
- **Erreurs** : -95% (automation = no human error)
- **Proactivit√©** : Alertes avant incidents$BODY$,
    'Automatisez DevOps avec Python. Backups, cleanups, health checks, notifications. Scripts production-ready, cron automation.',
    '/images/tutorials/python-automation.svg',
    'Automation',
    ARRAY['Python', 'Automation', 'DevOps', 'Scripts', 'Cron'],
    'published',
    NOW() - INTERVAL '14 days',
    289,
    23,
    'Python DevOps : 10 Scripts Automation Essentiels',
    'Python automation DevOps. Backups, cleanups, monitoring. Scripts production-ready.',
    ARRAY['python', 'automation', 'devops', 'scripts', 'cron']
);

-- AUTOMATION 2: Bash Scripts
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'Bash : Scripts Shell Avanc√©s pour DevOps Automation',
    'bash-shell-scripts-devops-automation',
    $BODY$# Bash Scripts DevOps

## üéØ Use Case : Deployment Script One-Click

D√©ploiement manuel : 20 commandes. Bash script : 1 commande = d√©ploiement complet.

## Deploy Script

```bash
#!/bin/bash
set -euo pipefail

# Configuration
APP_NAME="myapp"
VERSION="${1:-latest}"
ENVIRONMENT="${2:-production}"

echo "üöÄ Deploying $APP_NAME:$VERSION to $ENVIRONMENT"

# Backup actuel
echo "üì¶ Creating backup..."
kubectl get deployment $APP_NAME -o yaml > backup-$(date +%Y%m%d).yaml

# Build Docker image
echo "üê≥ Building image..."
docker build -t $APP_NAME:$VERSION .

# Push vers registry
echo "üì§ Pushing to registry..."
docker push registry.example.com/$APP_NAME:$VERSION

# Deploy Kubernetes
echo "‚ò∏Ô∏è  Deploying to Kubernetes..."
kubectl set image deployment/$APP_NAME $APP_NAME=registry.example.com/$APP_NAME:$VERSION

# Wait rollout
echo "‚è≥ Waiting for rollout..."
kubectl rollout status deployment/$APP_NAME

# Health check
echo "üè• Health check..."
for i in {1..30}; do
    if curl -sf http://api.example.com/health > /dev/null; then
        echo "‚úÖ Deployment successful!"
        exit 0
    fi
    sleep 2
done

echo "‚ùå Health check failed!"
echo "üîÑ Rolling back..."
kubectl rollout undo deployment/$APP_NAME
exit 1
```

## Usage

```bash
# Deploy version sp√©cifique
./deploy.sh v1.2.3 production

# Deploy latest
./deploy.sh latest staging
```

## Script Monitoring

```bash
#!/bin/bash

# Check services
services=("nginx" "postgresql" "redis")

for service in "${services[@]}"; do
    if systemctl is-active --quiet $service; then
        echo "‚úÖ $service is running"
    else
        echo "‚ùå $service is DOWN"
        systemctl restart $service

        # Notification Slack
        curl -X POST https://hooks.slack.com/services/YOUR/WEBHOOK \
          -d "{\"text\":\"‚ö†Ô∏è $service was down and has been restarted\"}"
    fi
done
```

## ROI

- **D√©ploiements** : 20 √©tapes ‚Üí 1 commande
- **Fiabilit√©** : Script = toujours m√™me process
- **Rollback** : Automatique en cas d'√©chec$BODY$,
    'Bash scripts avanc√©s pour DevOps. D√©ploiements one-click, health checks, rollback auto. Production-ready shell automation.',
    '/images/tutorials/bash-automation.svg',
    'Automation',
    ARRAY['Bash', 'Shell', 'Automation', 'DevOps', 'Scripts'],
    'published',
    NOW() - INTERVAL '10 days',
    223,
    21,
    'Bash Scripts DevOps : Automation et Deployment',
    'Bash automation DevOps. Scripts d√©ploiement, monitoring, rollback. Shell avanc√©.',
    ARRAY['bash', 'shell', 'automation', 'devops', 'scripts']
);

-- AUTOMATION 3: ChatOps Slack
INSERT INTO blog_posts (
    user_id, title, slug, content, excerpt, cover_image, category, tags,
    status, published_at, views, read_time, seo_title, seo_description, seo_keywords
) VALUES (
    '3cd1dbe8-35c8-4eb3-8e91-6d1e899028c3',
    'ChatOps : Automatiser DevOps via Slack Bot',
    'chatops-slack-bot-devops-automation',
    $BODY$# ChatOps avec Slack Bot

## üéØ Use Case : D√©ployer depuis Slack

√âquipe DevOps. Besoin : d√©ployer, scaler, rollback sans quitter Slack. Slack bot = interface DevOps.

## Architecture

```
Slack ‚Üí Bot (Node.js) ‚Üí Kubernetes API
              ‚Üì
        Notifications ‚Üê CI/CD Pipeline
```

## Slack Bot Node.js

```javascript
const { App } = require('@slack/bolt');

const app = new App({
  token: process.env.SLACK_BOT_TOKEN,
  signingSecret: process.env.SLACK_SIGNING_SECRET
});

// Commande: /deploy
app.command('/deploy', async ({ command, ack, respond }) => {
  await ack();

  const [service, version] = command.text.split(' ');

  await respond(`üöÄ D√©ploiement de ${service}:${version} en cours...`);

  try {
    // D√©ploiement K8s
    await kubectl.setImage(`deployment/${service}`, `${service}=${version}`);
    await respond(`‚úÖ ${service}:${version} d√©ploy√© avec succ√®s!`);
  } catch (error) {
    await respond(`‚ùå Erreur: ${error.message}`);
  }
});

// Commande: /scale
app.command('/scale', async ({ command, ack, respond }) => {
  await ack();

  const [service, replicas] = command.text.split(' ');

  await kubectl.scale(`deployment/${service}`, replicas);
  await respond(`‚úÖ ${service} scal√© √† ${replicas} replicas`);
});

// Commande: /status
app.command('/status', async ({ command, ack, respond }) => {
  await ack();

  const status = await kubectl.getDeployments();

  const message = status.map(d =>
    `‚Ä¢ ${d.name}: ${d.ready}/${d.replicas} ready`
  ).join('\n');

  await respond(`üìä Status:\n${message}`);
});

app.start(3000);
```

## Webhooks CI/CD

```javascript
// Recevoir notifs CI/CD
app.post('/webhooks/cicd', async (req, res) => {
  const { status, service, version } = req.body;

  const emoji = status === 'success' ? '‚úÖ' : '‚ùå';

  await app.client.chat.postMessage({
    channel: '#deployments',
    text: `${emoji} ${service}:${version} - ${status}`
  });

  res.sendStatus(200);
});
```

## Usage Slack

```
/deploy api v1.2.3
/scale worker 10
/rollback api
/status
/logs api --tail 50
```

## ROI

- **Accessibilit√©** : DevOps depuis mobile
- **Collaboration** : Toute l'√©quipe voit les d√©ploiements
- **Audit** : Historique complet dans Slack$BODY$,
    'ChatOps Slack bot pour DevOps. D√©ployez, scalez, monitorez depuis Slack. Collaboration √©quipe, audit automatique.',
    '/images/tutorials/chatops-slack.svg',
    'Automation',
    ARRAY['ChatOps', 'Slack', 'Bot', 'Automation', 'DevOps'],
    'published',
    NOW() - INTERVAL '5 days',
    312,
    26,
    'ChatOps Slack : Automatiser DevOps via Bot',
    'ChatOps Slack bot. D√©ploiements, scaling, monitoring depuis Slack. Collaboration √©quipe.',
    ARRAY['chatops', 'slack', 'bot', 'automation', 'devops']
);

-- ========================================
-- V√âRIFICATION FINALE
-- ========================================

-- Afficher statistiques
SELECT
    'üìä STATISTIQUES' as section,
    COUNT(*) as total_tutoriels,
    COUNT(*) FILTER (WHERE status = 'published') as publies
FROM blog_posts;

-- Compter par cat√©gorie
SELECT
    'üìÇ PAR CAT√âGORIE' as section,
    category,
    COUNT(*) as nombre
FROM blog_posts
GROUP BY category
ORDER BY nombre DESC;

-- Message de succ√®s
DO $$
BEGIN
    RAISE NOTICE '============================================';
    RAISE NOTICE '‚úÖ SUCC√àS : 28 TUTORIELS INS√âR√âS';
    RAISE NOTICE '============================================';
    RAISE NOTICE 'Cloud: 4 | DevOps: 4 | Kubernetes: 4';
    RAISE NOTICE 'CI/CD: 4 | Terraform: 3 | Ansible: 3';
    RAISE NOTICE 'Monitoring: 3 | Automation: 3';
    RAISE NOTICE '============================================';
END $$;
