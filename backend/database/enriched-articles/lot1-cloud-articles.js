// Lot 1: 3 premiers articles Cloud enrichis
export const lot1Articles = {
    'multi-cloud-terraform-aws-azure-gcp': {
        content: `# Multi-Cloud : DÃ©ployer sur AWS, Azure et GCP avec Terraform

## ğŸ¯ Use Case : Ã‰viter le Vendor Lock-In et Maximiser la RÃ©silience

Votre entreprise veut Ã©viter la dÃ©pendance Ã  un seul cloud provider. La stratÃ©gie multi-cloud permet de : (1) NÃ©gocier de meilleurs prix, (2) Utiliser les meilleurs services de chaque cloud, (3) Garantir la disponibilitÃ© en cas de panne majeure d'un provider, (4) Respecter les contraintes de rÃ©sidence des donnÃ©es (RGPD, souverainetÃ©).

**Contexte rÃ©el** : Une fintech europÃ©enne dÃ©ploie son infrastructure sur AWS (compute principal), Azure (Active Directory intÃ©gration), et GCP (BigQuery analytics). Terraform unifie la gestion des 3 clouds avec un seul langage IaC.

## ğŸ“‹ Architecture Multi-Cloud

### Distribution des Services

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  AWS (Primary)                      â”‚
â”‚  â€¢ EKS Kubernetes Clusters                         â”‚
â”‚  â€¢ RDS PostgreSQL Databases                        â”‚
â”‚  â€¢ S3 Storage                                      â”‚
â”‚  â€¢ CloudFront CDN                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Azure (Identity)                   â”‚
â”‚  â€¢ Azure AD (Single Sign-On)                       â”‚
â”‚  â€¢ Azure DevOps (CI/CD)                            â”‚
â”‚  â€¢ Azure Functions (Serverless)                    â”‚
â”‚  â€¢ Cosmos DB (Global distribution)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  GCP (Analytics)                    â”‚
â”‚  â€¢ BigQuery (Data Warehouse)                       â”‚
â”‚  â€¢ Cloud Functions (Data Processing)               â”‚
â”‚  â€¢ Cloud Storage (Data Lake)                       â”‚
â”‚  â€¢ Dataflow (ETL Pipelines)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

## ğŸ—ï¸ Structure Terraform Multi-Cloud

### Organisation des Dossiers

\`\`\`
terraform/
â”œâ”€â”€ main.tf                 # Point d'entrÃ©e principal
â”œâ”€â”€ variables.tf            # Variables globales
â”œâ”€â”€ outputs.tf              # Outputs consolidÃ©s
â”œâ”€â”€ providers.tf            # Configuration des 3 providers
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ aws/
â”‚   â”‚   â”œâ”€â”€ eks/           # EKS cluster module
â”‚   â”‚   â”œâ”€â”€ rds/           # RDS database module
â”‚   â”‚   â””â”€â”€ s3/            # S3 bucket module
â”‚   â”œâ”€â”€ azure/
â”‚   â”‚   â”œâ”€â”€ aks/           # AKS cluster module
â”‚   â”‚   â”œâ”€â”€ ad/            # Azure AD module
â”‚   â”‚   â””â”€â”€ functions/     # Azure Functions module
â”‚   â””â”€â”€ gcp/
â”‚       â”œâ”€â”€ gke/           # GKE cluster module
â”‚       â”œâ”€â”€ bigquery/      # BigQuery module
â”‚       â””â”€â”€ functions/     # Cloud Functions module
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ dev/
â”‚   â”œâ”€â”€ staging/
â”‚   â””â”€â”€ production/
â””â”€â”€ terraform.tfvars       # Variables d'environnement
\`\`\`

### providers.tf - Configuration Multi-Cloud

\`\`\`hcl
terraform {
  required_version = ">= 1.6"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0"
    }
    google = {
      source  = "hashicorp/google"
      version = "~> 5.0"
    }
  }

  # Remote state S3 (ou Azure Blob, ou GCS)
  backend "s3" {
    bucket         = "mycompany-terraform-state"
    key            = "multi-cloud/terraform.tfstate"
    region         = "eu-west-1"
    encrypt        = true
    dynamodb_table = "terraform-locks"
  }
}

# Provider AWS
provider "aws" {
  region = var.aws_region

  default_tags {
    tags = {
      Project     = "multi-cloud"
      ManagedBy   = "terraform"
      Environment = var.environment
    }
  }
}

# Provider Azure
provider "azurerm" {
  features {
    resource_group {
      prevent_deletion_if_contains_resources = true
    }
  }

  subscription_id = var.azure_subscription_id
  tenant_id       = var.azure_tenant_id
}

# Provider GCP
provider "google" {
  project = var.gcp_project_id
  region  = var.gcp_region

  default_labels = {
    project     = "multi-cloud"
    managed_by  = "terraform"
    environment = var.environment
  }
}
\`\`\`

## â˜ï¸ DÃ©ploiement AWS avec Terraform

### Module EKS

\`\`\`hcl
# modules/aws/eks/main.tf
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 19.0"

  cluster_name    = "multicloud-eks-\${var.environment}"
  cluster_version = "1.28"

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets

  # Managed Node Groups
  eks_managed_node_groups = {
    general = {
      desired_size = 3
      min_size     = 2
      max_size     = 10

      instance_types = ["t3.large"]
      capacity_type  = "SPOT"

      labels = {
        role = "general"
        cloud = "aws"
      }

      taints = []
    }

    compute = {
      desired_size = 2
      min_size     = 1
      max_size     = 20

      instance_types = ["c6i.xlarge"]
      capacity_type  = "ON_DEMAND"

      labels = {
        role = "compute-intensive"
        cloud = "aws"
      }
    }
  }

  # Cluster access
  cluster_endpoint_public_access = true

  # IRSA (IAM Roles for Service Accounts)
  enable_irsa = true

  tags = {
    Name  = "multicloud-eks"
    Cloud = "AWS"
  }
}

# RDS PostgreSQL
resource "aws_db_instance" "main" {
  identifier = "multicloud-db-\${var.environment}"

  engine         = "postgres"
  engine_version = "15.4"
  instance_class = "db.r6g.large"

  allocated_storage     = 100
  max_allocated_storage = 500
  storage_encrypted     = true

  db_name  = "multicloud"
  username = "admin"
  password = random_password.db_password.result

  multi_az = true

  backup_retention_period = 7
  skip_final_snapshot    = false

  tags = {
    Cloud = "AWS"
  }
}
\`\`\`

## â˜ï¸ DÃ©ploiement Azure avec Terraform

### Module AKS + Azure AD

\`\`\`hcl
# modules/azure/aks/main.tf
resource "azurerm_resource_group" "main" {
  name     = "rg-multicloud-\${var.environment}"
  location = var.azure_location

  tags = {
    cloud = "Azure"
  }
}

resource "azurerm_kubernetes_cluster" "main" {
  name                = "aks-multicloud-\${var.environment}"
  location            = azurerm_resource_group.main.location
  resource_group_name = azurerm_resource_group.main.name
  dns_prefix          = "multicloud-\${var.environment}"

  kubernetes_version = "1.28"

  default_node_pool {
    name       = "default"
    node_count = 3
    vm_size    = "Standard_D4s_v3"

    enable_auto_scaling = true
    min_count           = 2
    max_count           = 10

    os_disk_size_gb = 100
    type            = "VirtualMachineScaleSets"

    tags = {
      cloud = "Azure"
    }
  }

  # Azure AD Integration
  azure_active_directory_role_based_access_control {
    managed                = true
    admin_group_object_ids = [var.azure_ad_admin_group_id]
    azure_rbac_enabled     = true
  }

  # Managed Identity
  identity {
    type = "SystemAssigned"
  }

  network_profile {
    network_plugin    = "azure"
    load_balancer_sku = "standard"
    outbound_type     = "loadBalancer"
  }

  oms_agent {
    log_analytics_workspace_id = azurerm_log_analytics_workspace.main.id
  }

  tags = {
    cloud = "Azure"
  }
}

# Azure Functions pour processing
resource "azurerm_function_app" "processor" {
  name                       = "func-processor-\${var.environment}"
  location                   = azurerm_resource_group.main.location
  resource_group_name        = azurerm_resource_group.main.name
  app_service_plan_id        = azurerm_app_service_plan.main.id
  storage_account_name       = azurerm_storage_account.main.name
  storage_account_access_key = azurerm_storage_account.main.primary_access_key
  os_type                    = "linux"
  version                    = "~4"

  app_settings = {
    FUNCTIONS_WORKER_RUNTIME = "node"
    NODE_VERSION             = "18"
  }

  tags = {
    cloud = "Azure"
  }
}
\`\`\`

## â˜ï¸ DÃ©ploiement GCP avec Terraform

### Module GKE + BigQuery

\`\`\`hcl
# modules/gcp/gke/main.tf
resource "google_container_cluster" "main" {
  name     = "gke-multicloud-\${var.environment}"
  location = var.gcp_region

  # Remove default node pool
  remove_default_node_pool = true
  initial_node_count       = 1

  # Network
  network    = google_compute_network.main.name
  subnetwork = google_compute_subnetwork.main.name

  # IP allocation
  ip_allocation_policy {
    cluster_ipv4_cidr_block  = "/16"
    services_ipv4_cidr_block = "/22"
  }

  # Workload Identity
  workload_identity_config {
    workload_pool = "\${var.gcp_project_id}.svc.id.goog"
  }

  # GKE Autopilot (option)
  # enable_autopilot = true

  addons_config {
    horizontal_pod_autoscaling {
      disabled = false
    }
    http_load_balancing {
      disabled = false
    }
  }

  labels = {
    cloud = "gcp"
  }
}

resource "google_container_node_pool" "primary" {
  name       = "primary-pool"
  location   = var.gcp_region
  cluster    = google_container_cluster.main.name
  node_count = 3

  autoscaling {
    min_node_count = 2
    max_node_count = 10
  }

  node_config {
    machine_type = "n2-standard-4"
    disk_size_gb = 100
    disk_type    = "pd-standard"

    oauth_scopes = [
      "https://www.googleapis.com/auth/cloud-platform"
    ]

    labels = {
      cloud = "gcp"
    }

    workload_metadata_config {
      mode = "GKE_METADATA"
    }
  }
}

# BigQuery Data Warehouse
resource "google_bigquery_dataset" "analytics" {
  dataset_id  = "multicloud_analytics"
  location    = "EU"
  description = "Multi-cloud analytics dataset"

  default_table_expiration_ms = 3600000

  labels = {
    cloud = "gcp"
  }
}

resource "google_bigquery_table" "events" {
  dataset_id = google_bigquery_dataset.analytics.dataset_id
  table_id   = "events"

  schema = <<EOF
[
  {
    "name": "event_id",
    "type": "STRING",
    "mode": "REQUIRED"
  },
  {
    "name": "timestamp",
    "type": "TIMESTAMP",
    "mode": "REQUIRED"
  },
  {
    "name": "user_id",
    "type": "STRING",
    "mode": "NULLABLE"
  },
  {
    "name": "event_type",
    "type": "STRING",
    "mode": "REQUIRED"
  },
  {
    "name": "properties",
    "type": "JSON",
    "mode": "NULLABLE"
  }
]
EOF

  time_partitioning {
    type  = "DAY"
    field = "timestamp"
  }

  clustering = ["event_type", "user_id"]
}
\`\`\`

## ğŸ”„ Interconnexion Multi-Cloud

### VPN Site-to-Site

\`\`\`hcl
# AWS VPN Gateway
resource "aws_vpn_gateway" "main" {
  vpc_id = module.vpc.vpc_id

  tags = {
    Name = "multicloud-vpn-gw"
  }
}

# Azure VPN Gateway
resource "azurerm_virtual_network_gateway" "main" {
  name                = "vnet-gw-multicloud"
  location            = azurerm_resource_group.main.location
  resource_group_name = azurerm_resource_group.main.name

  type     = "Vpn"
  vpn_type = "RouteBased"
  sku      = "VpnGw2"

  ip_configuration {
    name                          = "vnetGatewayConfig"
    public_ip_address_id          = azurerm_public_ip.vpn.id
    private_ip_address_allocation = "Dynamic"
    subnet_id                     = azurerm_subnet.gateway.id
  }
}

# GCP VPN Gateway
resource "google_compute_vpn_gateway" "main" {
  name    = "vpn-gw-multicloud"
  network = google_compute_network.main.id
  region  = var.gcp_region
}

# AWS to Azure Connection
resource "aws_vpn_connection" "aws_to_azure" {
  vpn_gateway_id      = aws_vpn_gateway.main.id
  customer_gateway_id = aws_customer_gateway.azure.id
  type                = "ipsec.1"
  static_routes_only  = false
}
\`\`\`

## ğŸ“Š Monitoring Multi-Cloud UnifiÃ©

### Prometheus Federation

\`\`\`yaml
# prometheus-federation.yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  # AWS EKS
  - job_name: 'aws-eks'
    kubernetes_sd_configs:
      - role: pod
        api_server: 'https://eks-endpoint.amazonaws.com'
    relabel_configs:
      - source_labels: [__meta_kubernetes_namespace]
        action: keep
        regex: 'production'

  # Azure AKS
  - job_name: 'azure-aks'
    kubernetes_sd_configs:
      - role: pod
        api_server: 'https://aks-endpoint.azure.com'
    relabel_configs:
      - source_labels: [__meta_kubernetes_namespace]
        action: keep
        regex: 'production'

  # GCP GKE
  - job_name: 'gcp-gke'
    kubernetes_sd_configs:
      - role: pod
        api_server: 'https://gke-endpoint.gcp.com'
    relabel_configs:
      - source_labels: [__meta_kubernetes_namespace]
        action: keep
        regex: 'production'
\`\`\`

### Grafana Dashboard Multi-Cloud

\`\`\`json
{
  "dashboard": {
    "title": "Multi-Cloud Overview",
    "panels": [
      {
        "title": "Requests by Cloud Provider",
        "targets": [
          {
            "expr": "sum by (cloud) (rate(http_requests_total[5m]))"
          }
        ]
      },
      {
        "title": "Latency Comparison",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum by (cloud, le) (rate(http_request_duration_seconds_bucket[5m])))"
          }
        ]
      },
      {
        "title": "Cost by Provider (estimated)",
        "targets": [
          {
            "expr": "sum by (cloud) (cloud_cost_usd)"
          }
        ]
      }
    ]
  }
}
\`\`\`

## ğŸ’° Gestion des CoÃ»ts Multi-Cloud

### Cloud Cost Optimization

\`\`\`hcl
# AWS Cost Anomaly Detection
resource "aws_ce_anomaly_monitor" "main" {
  name              = "multicloud-cost-monitor"
  monitor_type      = "DIMENSIONAL"
  monitor_dimension = "SERVICE"
}

resource "aws_ce_anomaly_subscription" "main" {
  name      = "cost-anomaly-alerts"
  frequency = "DAILY"

  monitor_arn_list = [
    aws_ce_anomaly_monitor.main.arn
  ]

  subscriber {
    type    = "EMAIL"
    address = "devops@company.com"
  }

  threshold_expression {
    dimension {
      key           = "ANOMALY_TOTAL_IMPACT_ABSOLUTE"
      values        = ["100"]
      match_options = ["GREATER_THAN_OR_EQUAL"]
    }
  }
}

# Azure Cost Management Budget
resource "azurerm_consumption_budget_resource_group" "main" {
  name              = "multicloud-budget"
  resource_group_id = azurerm_resource_group.main.id

  amount     = 5000
  time_grain = "Monthly"

  time_period {
    start_date = "2024-01-01T00:00:00Z"
  }

  notification {
    enabled        = true
    threshold      = 80
    operator       = "GreaterThan"
    threshold_type = "Actual"

    contact_emails = [
      "devops@company.com"
    ]
  }
}
\`\`\`

## ğŸš¨ Disaster Recovery Multi-Cloud

### StratÃ©gie de Failover

\`\`\`hcl
# Global Traffic Manager (Azure)
resource "azurerm_traffic_manager_profile" "main" {
  name                   = "tm-multicloud"
  resource_group_name    = azurerm_resource_group.main.name
  traffic_routing_method = "Priority"

  dns_config {
    relative_name = "multicloud"
    ttl           = 30
  }

  monitor_config {
    protocol                     = "HTTPS"
    port                         = 443
    path                         = "/health"
    interval_in_seconds          = 30
    timeout_in_seconds           = 10
    tolerated_number_of_failures = 3
  }
}

# Endpoint AWS (Priority 1)
resource "azurerm_traffic_manager_endpoint" "aws" {
  name                = "aws-primary"
  resource_group_name = azurerm_resource_group.main.name
  profile_name        = azurerm_traffic_manager_profile.main.name
  type                = "externalEndpoints"
  target              = aws_lb.main.dns_name
  priority            = 1
  weight              = 100
}

# Endpoint Azure (Priority 2 - Fallback)
resource "azurerm_traffic_manager_endpoint" "azure" {
  name                = "azure-fallback"
  resource_group_name = azurerm_resource_group.main.name
  profile_name        = azurerm_traffic_manager_profile.main.name
  type                = "azureEndpoints"
  target_resource_id  = azurerm_public_ip.aks_ingress.id
  priority            = 2
  weight              = 100
}

# Endpoint GCP (Priority 3 - Last resort)
resource "azurerm_traffic_manager_endpoint" "gcp" {
  name                = "gcp-lastresort"
  resource_group_name = azurerm_resource_group.main.name
  profile_name        = azurerm_traffic_manager_profile.main.name
  type                = "externalEndpoints"
  target              = google_compute_global_address.main.address
  priority            = 3
  weight              = 100
}
\`\`\`

## ğŸ“ˆ ROI Multi-Cloud

### Avant Multi-Cloud (Single Cloud)
- ğŸ’° **CoÃ»ts** : $50K/mois (AWS uniquement)
- âš ï¸ **Vendor Lock-in** : 100% dÃ©pendance AWS
- ğŸ”§ **NÃ©gociation** : Pouvoir limitÃ© sur pricing
- ğŸ“Š **Services** : LimitÃ© aux services AWS
- âŒ **RÃ©silience** : Panne AWS = service down

### AprÃ¨s Multi-Cloud
- ğŸ’° **CoÃ»ts** : $42K/mois (-16%, meilleur pricing nÃ©gociÃ©)
- âœ… **Vendor Lock-in** : 0%, migration facile
- ğŸ”§ **NÃ©gociation** : Fort pouvoir, -20% sur contrats
- ğŸ“Š **Services** : Meilleurs services de chaque cloud
- âœ… **RÃ©silience** : Panne 1 cloud = failover automatique

### MÃ©triques Business
- **Availability** : 99.95% â†’ 99.995% (x10 moins de downtime)
- **Negotiation Power** : +40% de rÃ©duction sur contrats
- **Time to Market** : -30% (meilleurs services par use case)
- **Vendor Independence** : 100% libertÃ© de migration

## ğŸ”— Ressources

- [Terraform Multi-Cloud Best Practices](https://www.terraform.io/docs/cloud/multi-cloud/)
- [AWS-Azure-GCP Service Comparison](https://cloud.google.com/docs/compare/aws)
- [Multi-Cloud Architecture Patterns](https://docs.microsoft.com/azure/architecture/guide/technology-choices/multi-cloud)
- [Cloud Comparison Tool](https://www.cloudhealthtech.com/)`,
        read_time: 16
    },

    'docker-multi-stage-builds-optimization': {
        content: `# Docker Multi-Stage Builds : RÃ©duire vos Images de 1GB Ã  50MB

## ğŸ¯ Use Case : Optimiser la Taille des Images et le Temps de DÃ©ploiement

Votre application Node.js pÃ¨se 1.2GB en image Docker. Le push vers le registry prend 10 minutes, le pull sur chaque serveur 8 minutes, et les scans de sÃ©curitÃ© timeoutent. En utilisant multi-stage builds, vous rÃ©duisez l'image Ã  50MB : push en 30 secondes, pull en 20 secondes, scans rapides, et Ã©conomie de 95% sur les coÃ»ts de stockage/transfer.

**Contexte rÃ©el** : Une application dÃ©ployÃ©e sur 100 pods Kubernetes. Avec 1.2GB/image : 120GB de pulls Ã  chaque dÃ©ploiement = 30 minutes. Avec 50MB : 5GB = 2 minutes. Ã‰conomie de temps et de bande passante massive.

## ğŸ“Š Avant/AprÃ¨s Multi-Stage

### Image Simple (Sans Multi-Stage)

\`\`\`dockerfile
FROM node:20

WORKDIR /app

# Copier TOUT (y compris node_modules dev, tests, .git, etc.)
COPY . .

# Installer TOUTES les dÃ©pendances (prod + dev)
RUN npm install

# Build
RUN npm run build

# Exposer port
EXPOSE 3000

CMD ["npm", "start"]
\`\`\`

**RÃ©sultat** :
- Taille : **1.2GB**
- Contient : Node.js complet, npm, dev dependencies, tests, source files, .git, etc.
- Layers : 15 layers
- VulnÃ©rabilitÃ©s : 47 CVE (beaucoup viennent des dev dependencies)

### Image Multi-Stage OptimisÃ©e

\`\`\`dockerfile
# Stage 1: Builder
FROM node:20-alpine AS builder

WORKDIR /app

# Copier uniquement package files
COPY package*.json ./

# Install dependencies (prod + dev)
RUN npm ci

# Copier source code
COPY . .

# Build application
RUN npm run build

# Stage 2: Runtime
FROM node:20-alpine AS runtime

WORKDIR /app

# Copier uniquement package files
COPY package*.json ./

# Install ONLY production dependencies
RUN npm ci --only=production && \\
    npm cache clean --force

# Copier les artifacts buildÃ©s depuis stage 1
COPY --from=builder /app/dist ./dist

# Non-root user
RUN addgroup -g 1001 -S nodejs && \\
    adduser -S nodejs -u 1001 && \\
    chown -R nodejs:nodejs /app

USER nodejs

EXPOSE 3000

CMD ["node", "dist/index.js"]
\`\`\`

**RÃ©sultat** :
- Taille : **50MB** (-96%)
- Contient : Node.js runtime, production dependencies, compiled code uniquement
- Layers : 8 layers
- VulnÃ©rabilitÃ©s : 3 CVE (-94%)

## ğŸ—ï¸ Anatomie d'un Multi-Stage Build

### Stage 1 : Build

Le premier stage contient **tout ce qui est nÃ©cessaire pour compiler** :

\`\`\`dockerfile
# Base image avec tous les outils de build
FROM node:20 AS builder

# Variables d'environnement de build
ARG BUILD_ENV=production
ENV NODE_ENV=production

WORKDIR /app

# Optimisation : Copier package.json AVANT le code
# Si le code change mais pas les deps, le layer est cachÃ©
COPY package*.json ./
COPY package-lock.json ./

# Install TOUTES les dÃ©pendances (y compris devDependencies)
# Car on a besoin de TypeScript, Webpack, etc. pour compiler
RUN npm ci --include=dev

# Copier le code source
COPY tsconfig.json ./
COPY src ./src
COPY public ./public

# Build
RUN npm run build && \\
    npm run test:ci && \\
    npm audit --audit-level=high
\`\`\`

### Stage 2 : Runtime

Le deuxiÃ¨me stage est **minimal, ne contient que ce qui est nÃ©cessaire Ã  l'exÃ©cution** :

\`\`\`dockerfile
# Base image minimale (alpine = 5MB vs standard = 900MB)
FROM node:20-alpine AS runtime

# Metadata
LABEL maintainer="devops@company.com"
LABEL version="1.0.0"

WORKDIR /app

# Copier UNIQUEMENT package files
COPY package*.json ./

# Install UNIQUEMENT production dependencies
RUN npm ci --only=production && \\
    npm cache clean --force && \\
    rm -rf /tmp/*

# Copier les artifacts buildÃ©s depuis le stage "builder"
COPY --from=builder /app/dist ./dist
COPY --from=builder /app/public ./public

# SÃ©curitÃ© : User non-root
RUN addgroup -g 1001 -S nodejs && \\
    adduser -S nodejs -u 1001 && \\
    chown -R nodejs:nodejs /app

USER nodejs

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\
  CMD node -e "require('http').get('http://localhost:3000/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"

EXPOSE 3000

CMD ["node", "dist/index.js"]
\`\`\`

## ğŸš€ Cas d'Usage par Langage

### Node.js avec TypeScript

\`\`\`dockerfile
# Stage 1: Builder
FROM node:20-alpine AS deps
WORKDIR /app
COPY package*.json ./
RUN npm ci

FROM node:20-alpine AS builder
WORKDIR /app
COPY --from=deps /app/node_modules ./node_modules
COPY . .
RUN npm run build

# Stage 2: Runner
FROM node:20-alpine AS runner
WORKDIR /app
ENV NODE_ENV production

RUN addgroup --system --gid 1001 nodejs && \\
    adduser --system --uid 1001 nextjs

COPY --from=builder /app/public ./public
COPY --from=builder --chown=nextjs:nodejs /app/.next/standalone ./
COPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static

USER nextjs

EXPOSE 3000
ENV PORT 3000

CMD ["node", "server.js"]
\`\`\`

### Python FastAPI

\`\`\`dockerfile
# Stage 1: Builder
FROM python:3.11-slim AS builder

WORKDIR /app

# Install build dependencies
RUN apt-get update && \\
    apt-get install -y --no-install-recommends \\
    gcc \\
    postgresql-dev && \\
    rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# Stage 2: Runtime
FROM python:3.11-slim

WORKDIR /app

# Copy dependencies from builder
COPY --from=builder /root/.local /root/.local

# Copy application
COPY . .

# Non-root user
RUN useradd -m -u 1000 appuser && \\
    chown -R appuser:appuser /app
USER appuser

# Make sure scripts in .local are usable
ENV PATH=/root/.local/bin:$PATH

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
\`\`\`

### Go Application

\`\`\`dockerfile
# Stage 1: Builder
FROM golang:1.21-alpine AS builder

WORKDIR /app

# Download dependencies
COPY go.mod go.sum ./
RUN go mod download

# Copy source
COPY . .

# Build static binary
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags="-w -s" -o main .

# Stage 2: Runtime
FROM scratch

# Copy SSL certificates (pour HTTPS calls)
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/

# Copy binary
COPY --from=builder /app/main /main

EXPOSE 8080

ENTRYPOINT ["/main"]
\`\`\`

**Note** : Go avec `scratch` = **2-10MB** seulement ! La plus petite image possible.

### Java Spring Boot

\`\`\`dockerfile
# Stage 1: Builder
FROM maven:3.9-eclipse-temurin-17 AS builder

WORKDIR /app

# Download dependencies (cached layer)
COPY pom.xml .
RUN mvn dependency:go-offline

# Copy source and build
COPY src ./src
RUN mvn clean package -DskipTests

# Stage 2: Runtime
FROM eclipse-temurin:17-jre-alpine

WORKDIR /app

# Copy JAR from builder
COPY --from=builder /app/target/*.jar app.jar

# Non-root user
RUN addgroup -S spring && adduser -S spring -G spring
USER spring:spring

EXPOSE 8080

ENTRYPOINT ["java", "-Djava.security.egd=file:/dev/./urandom", "-jar", "app.jar"]
\`\`\`

## ğŸ”§ Optimisations AvancÃ©es

### 1. Utiliser Distroless pour Maximum SÃ©curitÃ©

\`\`\`dockerfile
# Stage 1: Builder (normal)
FROM node:20 AS builder
WORKDIR /app
COPY package*.json ./
RUN npm ci
COPY . .
RUN npm run build

# Stage 2: Distroless (NO shell, NO package manager)
FROM gcr.io/distroless/nodejs20-debian11

WORKDIR /app

COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/dist ./dist

USER nonroot:nonroot

CMD ["dist/index.js"]
\`\`\`

**Avantages Distroless** :
- âŒ Pas de shell (impossible d'exec dans le container)
- âŒ Pas de package manager (apt, yum, etc.)
- âŒ Pas de binaires systÃ¨me inutiles
- âœ… Surface d'attaque minimale
- âœ… 50-80% plus petit que Alpine

### 2. BuildKit Cache Mounts

\`\`\`dockerfile
# syntax=docker/dockerfile:1.4

FROM node:20-alpine AS builder

WORKDIR /app

# Cache npm downloads entre builds
RUN --mount=type=cache,target=/root/.npm \\
    npm install

# Cache TypeScript compilation
RUN --mount=type=cache,target=/app/.tsbuildinfo \\
    npm run build
\`\`\`

**RÃ©sultat** : Build time divisÃ© par 3-5x

### 3. Multi-Architecture Builds

\`\`\`dockerfile
# Support AMD64 et ARM64 (Mac M1/M2, AWS Graviton)
FROM --platform=$BUILDPLATFORM node:20-alpine AS builder

ARG TARGETPLATFORM
ARG BUILDPLATFORM

RUN echo "Building for $TARGETPLATFORM on $BUILDPLATFORM"

WORKDIR /app
COPY . .
RUN npm ci && npm run build

FROM node:20-alpine
COPY --from=builder /app/dist ./dist
CMD ["node", "dist/index.js"]
\`\`\`

**Build multi-arch** :
\`\`\`bash
docker buildx build \\
  --platform linux/amd64,linux/arm64 \\
  -t myapp:latest \\
  --push .
\`\`\`

## ğŸ“Š Analyse de Taille d'Image

### Outils d'Analyse

\`\`\`bash
# Dive : analyser layers image
docker run --rm -it \\
  -v /var/run/docker.sock:/var/run/docker.sock \\
  wagoodman/dive:latest myapp:latest

# Docker history
docker history myapp:latest --no-trunc

# Export et analyse manuelle
docker save myapp:latest -o myapp.tar
tar -xvf myapp.tar
\`\`\`

### Exemple Analyse Dive

\`\`\`
â”ƒ Layer Details â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ Cmp   Size  Command
â”‚     5.6 MB  FROM node:20-alpine
â”‚     450 kB  RUN npm ci --only=production
â”‚      12 MB  COPY --from=builder /app/dist
â”‚     890 B  RUN adduser ...
â”‚     Total Image size: 18.1 MB
â”‚     Potential wasted space: 234 kB
â”‚     Image efficiency score: 98.7%
\`\`\`

## ğŸš¨ PiÃ¨ges Ã  Ã‰viter

### âŒ Mauvaise Pratique 1 : Copier AVANT npm install

\`\`\`dockerfile
# MAUVAIS : Casse le cache Ã  chaque changement de code
COPY . .
RUN npm install
\`\`\`

### âœ… Bonne Pratique : Copier package.json AVANT

\`\`\`dockerfile
# BON : Cache npm install si dependencies inchangÃ©es
COPY package*.json ./
RUN npm ci
COPY . .
\`\`\`

### âŒ Mauvaise Pratique 2 : Installer devDependencies en prod

\`\`\`dockerfile
# MAUVAIS : Image 3x plus grosse
RUN npm install
\`\`\`

### âœ… Bonne Pratique : --only=production

\`\`\`dockerfile
# BON : Seulement prod dependencies
RUN npm ci --only=production
\`\`\`

### âŒ Mauvaise Pratique 3 : Laisser des fichiers temporaires

\`\`\`dockerfile
# MAUVAIS : npm cache reste dans l'image
RUN npm install
RUN npm cache clean --force
\`\`\`

### âœ… Bonne Pratique : Tout dans un seul RUN

\`\`\`dockerfile
# BON : Cache nettoyÃ© dans la mÃªme layer
RUN npm ci --only=production && \\
    npm cache clean --force && \\
    rm -rf /tmp/*
\`\`\`

## ğŸ“ˆ ROI Multi-Stage Builds

### Avant Multi-Stage
- ğŸ“¦ **Taille image** : 1.2GB
- â±ï¸ **Push registry** : 10 minutes (100 Mbps)
- â±ï¸ **Pull sur serveur** : 8 minutes
- ğŸ’° **Storage costs** : $50/mois (100 images * 1.2GB)
- ğŸ’° **Transfer costs** : $150/mois (egress)
- ğŸ” **Scan time** : 5 minutes (timeouts frÃ©quents)
- âš ï¸ **VulnÃ©rabilitÃ©s** : 47 CVE

### AprÃ¨s Multi-Stage + Alpine
- ğŸ“¦ **Taille image** : 50MB (-96%)
- â±ï¸ **Push registry** : 30 secondes
- â±ï¸ **Pull sur serveur** : 20 secondes
- ğŸ’° **Storage costs** : $2/mois (-96%)
- ğŸ’° **Transfer costs** : $6/mois (-96%)
- ğŸ” **Scan time** : 15 secondes
- âœ… **VulnÃ©rabilitÃ©s** : 3 CVE (-94%)

### MÃ©triques Business
- **Deployment time** : 30 min â†’ 2 min (-93%)
- **CI/CD pipeline** : 45 min â†’ 8 min (-82%)
- **Bandwidth costs** : -96%
- **Storage costs** : -96%
- **Security posture** : +94% (moins de CVE)

## ğŸ”— Ressources

- [Docker Multi-Stage Builds](https://docs.docker.com/build/building/multi-stage/)
- [Best Practices for Dockerfile](https://docs.docker.com/develop/dev-best-practices/)
- [Distroless Images](https://github.com/GoogleContainerTools/distroless)
- [Dive - Image Analysis Tool](https://github.com/wagoodman/dive)`,
        read_time: 14
    },

    'docker-compose-microservices-local': {
        content: `# Docker Compose : Stack Microservices ComplÃ¨te en Local

## ğŸ¯ Use Case : DÃ©veloppement Local Multi-Services Sans Kubernetes

Votre application est composÃ©e de 8 microservices (API Gateway, Auth, Users, Orders, Payments, Notifications, Analytics, Background Workers) + 4 databases (PostgreSQL, Redis, MongoDB, Elasticsearch). Configurer tout Ã§a manuellement prend 2 heures et gÃ©nÃ¨re des erreurs. Docker Compose : 1 commande (\`docker-compose up\`) = environnement complet en 30 secondes.

**Contexte rÃ©el** : Onboarding de nouveaux dÃ©veloppeurs. Sans Docker Compose : 1 journÃ©e pour setup l'environnement + aide senior dev. Avec Docker Compose : 5 minutes autonomes, documentÃ© dans le code.

## ğŸ“‹ Architecture Stack ComplÃ¨te

### Services de l'Application

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FRONTEND                          â”‚
â”‚              React App (Port 3000)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                API GATEWAY                          â”‚
â”‚              nginx (Port 80/443)                    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚      â”‚       â”‚      â”‚      â”‚
â”Œâ”€â”€â”€â”€â”´â”€â”€â” â”Œâ”´â”€â”€â”€â”€â” â”Œâ”´â”€â”€â”€â”€â”â”Œâ”´â”€â”€â”€â”€â”â”Œâ”´â”€â”€â”€â”€â”€â”€â”
â”‚ Auth  â”‚ â”‚Usersâ”‚ â”‚Orderâ”‚â”‚Pay  â”‚â”‚Notifs â”‚
â”‚ :3001 â”‚ â”‚:3002â”‚ â”‚:3003â”‚â”‚:3004â”‚â”‚:3005  â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜â””â”€â”€â”¬â”€â”€â”˜â””â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚        â”‚       â”‚      â”‚       â”‚
â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”
â”‚         Databases & Message Queue       â”‚
â”‚  PostgreSQL | MongoDB | Redis | RabbitMQâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

## ğŸ³ docker-compose.yml Complet

\`\`\`yaml
version: '3.8'

services:
  # ===========================
  # INFRASTRUCTURE
  # ===========================

  postgres:
    image: postgres:15-alpine
    container_name: microservices-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: microservices
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin123
      POSTGRES_MULTIPLE_DATABASES: auth,users,orders,payments
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-databases.sh:/docker-entrypoint-initdb.d/init-databases.sh
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin"]
      interval: 10s
      timeout: 5s
      retries: 5

  mongodb:
    image: mongo:7
    container_name: microservices-mongo
    restart: unless-stopped
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: admin123
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: microservices-redis
    restart: unless-stopped
    command: redis-server --appendonly yes --requirepass redis123
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  rabbitmq:
    image: rabbitmq:3-management-alpine
    container_name: microservices-rabbitmq
    restart: unless-stopped
    environment:
      RABBITMQ_DEFAULT_USER: admin
      RABBITMQ_DEFAULT_PASS: admin123
    ports:
      - "5672:5672"    # AMQP
      - "15672:15672"  # Management UI
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: microservices-elasticsearch
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ===========================
  # API GATEWAY
  # ===========================

  api-gateway:
    build:
      context: ./services/api-gateway
      dockerfile: Dockerfile
    container_name: microservices-gateway
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./services/api-gateway/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./services/api-gateway/ssl:/etc/nginx/ssl:ro
    depends_on:
      - auth-service
      - users-service
      - orders-service
      - payments-service
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  # ===========================
  # MICROSERVICES
  # ===========================

  auth-service:
    build:
      context: ./services/auth
      dockerfile: Dockerfile
      target: development
    container_name: microservices-auth
    restart: unless-stopped
    environment:
      NODE_ENV: development
      PORT: 3001
      DATABASE_URL: postgresql://admin:admin123@postgres:5432/auth
      REDIS_URL: redis://:redis123@redis:6379
      JWT_SECRET: your-secret-key
    ports:
      - "3001:3001"
    volumes:
      - ./services/auth:/app
      - /app/node_modules
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - microservices-network
    command: npm run dev

  users-service:
    build:
      context: ./services/users
      dockerfile: Dockerfile
      target: development
    container_name: microservices-users
    restart: unless-stopped
    environment:
      NODE_ENV: development
      PORT: 3002
      DATABASE_URL: postgresql://admin:admin123@postgres:5432/users
      REDIS_URL: redis://:redis123@redis:6379
      AUTH_SERVICE_URL: http://auth-service:3001
    ports:
      - "3002:3002"
    volumes:
      - ./services/users:/app
      - /app/node_modules
    depends_on:
      postgres:
        condition: service_healthy
      auth-service:
        condition: service_started
    networks:
      - microservices-network
    command: npm run dev

  orders-service:
    build:
      context: ./services/orders
      dockerfile: Dockerfile
      target: development
    container_name: microservices-orders
    restart: unless-stopped
    environment:
      NODE_ENV: development
      PORT: 3003
      DATABASE_URL: postgresql://admin:admin123@postgres:5432/orders
      MONGODB_URL: mongodb://admin:admin123@mongodb:27017/orders?authSource=admin
      RABBITMQ_URL: amqp://admin:admin123@rabbitmq:5672
      USERS_SERVICE_URL: http://users-service:3002
    ports:
      - "3003:3003"
    volumes:
      - ./services/orders:/app
      - /app/node_modules
    depends_on:
      postgres:
        condition: service_healthy
      mongodb:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - microservices-network
    command: npm run dev

  payments-service:
    build:
      context: ./services/payments
      dockerfile: Dockerfile
      target: development
    container_name: microservices-payments
    restart: unless-stopped
    environment:
      NODE_ENV: development
      PORT: 3004
      DATABASE_URL: postgresql://admin:admin123@postgres:5432/payments
      RABBITMQ_URL: amqp://admin:admin123@rabbitmq:5672
      STRIPE_API_KEY: ${STRIPE_API_KEY}
      STRIPE_WEBHOOK_SECRET: ${STRIPE_WEBHOOK_SECRET}
    ports:
      - "3004:3004"
    volumes:
      - ./services/payments:/app
      - /app/node_modules
    depends_on:
      postgres:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - microservices-network
    command: npm run dev

  notifications-service:
    build:
      context: ./services/notifications
      dockerfile: Dockerfile
      target: development
    container_name: microservices-notifications
    restart: unless-stopped
    environment:
      NODE_ENV: development
      PORT: 3005
      RABBITMQ_URL: amqp://admin:admin123@rabbitmq:5672
      SMTP_HOST: mailhog
      SMTP_PORT: 1025
      TWILIO_ACCOUNT_SID: ${TWILIO_ACCOUNT_SID}
      TWILIO_AUTH_TOKEN: ${TWILIO_AUTH_TOKEN}
    ports:
      - "3005:3005"
    volumes:
      - ./services/notifications:/app
      - /app/node_modules
    depends_on:
      rabbitmq:
        condition: service_healthy
    networks:
      - microservices-network
    command: npm run dev

  # Background worker pour async jobs
  worker:
    build:
      context: ./services/worker
      dockerfile: Dockerfile
      target: development
    container_name: microservices-worker
    restart: unless-stopped
    environment:
      NODE_ENV: development
      RABBITMQ_URL: amqp://admin:admin123@rabbitmq:5672
      REDIS_URL: redis://:redis123@redis:6379
    volumes:
      - ./services/worker:/app
      - /app/node_modules
    depends_on:
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - microservices-network
    command: npm run dev

  # ===========================
  # FRONTEND
  # ===========================

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: development
    container_name: microservices-frontend
    restart: unless-stopped
    environment:
      REACT_APP_API_URL: http://localhost
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    networks:
      - microservices-network
    command: npm start

  # ===========================
  # MONITORING & OBSERVABILITY
  # ===========================

  prometheus:
    image: prom/prometheus:latest
    container_name: microservices-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - microservices-network

  grafana:
    image: grafana/grafana:latest
    container_name: microservices-grafana
    restart: unless-stopped
    ports:
      - "3001:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin123
    volumes:
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - microservices-network

  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: microservices-jaeger
    restart: unless-stopped
    ports:
      - "5775:5775/udp"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "16686:16686"  # UI
      - "14268:14268"
      - "14250:14250"
    networks:
      - microservices-network

  # Mailhog pour tester emails en local
  mailhog:
    image: mailhog/mailhog:latest
    container_name: microservices-mailhog
    restart: unless-stopped
    ports:
      - "1025:1025"  # SMTP
      - "8025:8025"  # UI
    networks:
      - microservices-network

# ===========================
# NETWORKS
# ===========================

networks:
  microservices-network:
    driver: bridge

# ===========================
# VOLUMES
# ===========================

volumes:
  postgres_data:
  mongodb_data:
  redis_data:
  rabbitmq_data:
  elasticsearch_data:
  prometheus_data:
  grafana_data:
\`\`\`

## ğŸš€ Commandes Docker Compose Essentielles

### DÃ©marrage et ArrÃªt

\`\`\`bash
# DÃ©marrer tous les services
docker-compose up

# DÃ©marrer en arriÃ¨re-plan
docker-compose up -d

# DÃ©marrer seulement certains services
docker-compose up postgres redis auth-service

# ArrÃªter tous les services
docker-compose down

# ArrÃªter et supprimer volumes (reset complet)
docker-compose down -v

# Rebuild images avant de dÃ©marrer
docker-compose up --build
\`\`\`

### Logs et Debugging

\`\`\`bash
# Voir logs de tous les services
docker-compose logs

# Suivre logs en temps rÃ©el
docker-compose logs -f

# Logs d'un service spÃ©cifique
docker-compose logs -f auth-service

# Logs des 100 derniÃ¨res lignes
docker-compose logs --tail=100

# Exec dans un container
docker-compose exec auth-service sh
docker-compose exec postgres psql -U admin -d microservices
\`\`\`

### Scaling et Performance

\`\`\`bash
# Scaler un service
docker-compose up -d --scale worker=5

# Restart un service spÃ©cifique
docker-compose restart auth-service

# Recreate containers (sans rebuild)
docker-compose up -d --force-recreate

# Voir l'Ã©tat des services
docker-compose ps

# Voir les resources utilisÃ©es
docker-compose stats
\`\`\`

## ğŸ”§ Configuration AvancÃ©e

### Hot Reload en DÃ©veloppement

\`\`\`yaml
# Dockerfile multi-stage pour dev
FROM node:20-alpine AS base
WORKDIR /app
COPY package*.json ./
RUN npm install

FROM base AS development
# Install nodemon pour hot reload
RUN npm install -g nodemon
COPY . .
CMD ["nodemon", "--legacy-watch", "src/index.js"]

FROM base AS production
RUN npm ci --only=production
COPY . .
CMD ["node", "src/index.js"]
\`\`\`

### Variables d'Environnement Multiples

\`\`\`bash
# .env.local
DATABASE_URL=postgresql://admin:admin123@localhost:5432/myapp
REDIS_URL=redis://localhost:6379

# .env.docker
DATABASE_URL=postgresql://admin:admin123@postgres:5432/myapp
REDIS_URL=redis://redis:6379

# Utiliser .env.docker
docker-compose --env-file .env.docker up
\`\`\`

### Profils pour DiffÃ©rents Environnements

\`\`\`yaml
services:
  # Toujours actif
  postgres:
    image: postgres:15-alpine

  # Seulement en dev (avec --profile dev)
  pgadmin:
    image: dpage/pgadmin4
    profiles: ["dev"]
    ports:
      - "5050:80"

  # Seulement en test (avec --profile test)
  test-runner:
    build: ./tests
    profiles: ["test"]
    command: npm test
\`\`\`

\`\`\`bash
# Dev profile
docker-compose --profile dev up

# Test profile
docker-compose --profile test up
\`\`\`

## ğŸ“Š Monitoring Stack IntÃ©grÃ©

### Prometheus Config

\`\`\`yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'services'
    static_configs:
      - targets:
        - 'auth-service:3001'
        - 'users-service:3002'
        - 'orders-service:3003'
        - 'payments-service:3004'

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres:5432']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']
\`\`\`

### Grafana Dashboards

\`\`\`yaml
# monitoring/grafana/datasources/prometheus.yml
apiVersion: 1
datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
\`\`\`

## ğŸš¨ Troubleshooting

### Port DÃ©jÃ  UtilisÃ©

**Erreur** :
\`\`\`
Error starting container: Bind for 0.0.0.0:5432 failed: port is already allocated
\`\`\`

**Solutions** :
\`\`\`bash
# 1. Changer le port dans docker-compose.yml
ports:
  - "5433:5432"  # Host:Container

# 2. Trouver et killer le process
lsof -i :5432
kill -9 <PID>

# 3. Stopper Postgres local
sudo systemctl stop postgresql
\`\`\`

### Service Unhealthy

**Diagnostic** :
\`\`\`bash
# Voir health check status
docker-compose ps

# Logs du service
docker-compose logs postgres

# Exec et tester manuellement
docker-compose exec postgres pg_isready -U admin
\`\`\`

### Database Not Ready

**Solution** : Utiliser depends_on avec condition
\`\`\`yaml
depends_on:
  postgres:
    condition: service_healthy
\`\`\`

## ğŸ“ˆ ROI Docker Compose

### Avant Docker Compose
- â±ï¸ **Setup environnement** : 2 heures (nouveau dev)
- ğŸ”§ **Maintenance** : 5h/semaine (conflits de versions)
- âŒ **"Works on my machine"** : 3-4 fois/semaine
- ğŸ“š **Documentation** : 10 pages de setup manuel
- ğŸ‘¥ **Onboarding** : 1 journÃ©e avec senior dev

### AprÃ¨s Docker Compose
- â±ï¸ **Setup environnement** : 5 minutes (\`docker-compose up\`)
- ğŸ”§ **Maintenance** : 30 min/semaine
- âœ… **"Works on my machine"** : 0 (environnement identique)
- ğŸ“š **Documentation** : 1 fichier docker-compose.yml
- ğŸ‘¥ **Onboarding** : 15 minutes autonome

### MÃ©triques Business
- **Developer onboarding** : -95% de temps
- **Environment consistency** : 100% (mÃªme env partout)
- **Debug time** : -70% (env reproductible)
- **Senior dev time saved** : 4h/semaine/nouveau dev

## ğŸ”— Ressources

- [Docker Compose Documentation](https://docs.docker.com/compose/)
- [Awesome Compose Examples](https://github.com/docker/awesome-compose)
- [Compose Specification](https://compose-spec.io/)
- [Best Practices](https://docs.docker.com/compose/production/)`,
        read_time: 15
    }
};
