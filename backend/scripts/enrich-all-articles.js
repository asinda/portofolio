import supabase from '../src/config/supabase.js';

// Articles enrichis avec contenu complet
const enrichedArticles = {
    'aws-architecture-3-tiers': {
        content: `# AWS : DÃ©ployer une Architecture 3-Tiers Scalable

## ğŸ¯ Use Case : Application Web Haute DisponibilitÃ©

Votre startup SaaS doit dÃ©ployer une application web qui supporte 100K utilisateurs simultanÃ©s avec 99.99% uptime. L'architecture doit Ãªtre rÃ©siliente, scalable automatiquement, et distribuÃ©e sur plusieurs zones de disponibilitÃ© pour survivre Ã  une panne datacenter.

**Contexte rÃ©el** : Une plateforme e-commerce traite 1M de transactions/jour. L'architecture 3-tiers classique (Web, App, DB) doit Ãªtre hautement disponible avec autoscaling, load balancing, et disaster recovery.

## ğŸ“‹ Architecture 3-Tiers sur AWS

### Composants

1. **Tier 1 - Web (Public Subnets)** : Application Load Balancer + Auto Scaling Group
2. **Tier 2 - Application (Private Subnets)** : EC2 instances avec business logic
3. **Tier 3 - Database (Private Subnets)** : RDS Multi-AZ avec read replicas

### SchÃ©ma Infrastructure

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Internet Gateway                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Application Load Balancer  â”‚  (Public)
      â”‚         Multi-AZ             â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Auto Scaling Group (Web)    â”‚  (Public)
      â”‚  EC2: Nginx + Static Assets  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Auto Scaling Group (App)    â”‚  (Private)
      â”‚  EC2: Node.js/Python/Java    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  RDS Multi-AZ PostgreSQL     â”‚  (Private)
      â”‚  Primary + Read Replicas     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

## ğŸ—ï¸ Terraform Infrastructure as Code

### vpc.tf - RÃ©seau VPC

\`\`\`hcl
# VPC avec 3 AZ pour haute disponibilitÃ©
resource "aws_vpc" "main" {
  cidr_block           = "10.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name        = "production-vpc"
    Environment = "production"
  }
}

# Internet Gateway
resource "aws_internet_gateway" "main" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = "production-igw"
  }
}

# Subnets publics (Web tier) - 3 AZ
resource "aws_subnet" "public" {
  count                   = 3
  vpc_id                  = aws_vpc.main.id
  cidr_block              = "10.0.\${count.index + 1}.0/24"
  availability_zone       = data.aws_availability_zones.available.names[count.index]
  map_public_ip_on_launch = true

  tags = {
    Name = "public-subnet-\${count.index + 1}"
    Tier = "web"
  }
}

# Subnets privÃ©s (App tier) - 3 AZ
resource "aws_subnet" "private_app" {
  count             = 3
  vpc_id            = aws_vpc.main.id
  cidr_block        = "10.0.\${count.index + 10}.0/24"
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = "private-app-subnet-\${count.index + 1}"
    Tier = "application"
  }
}

# Subnets privÃ©s (DB tier) - 3 AZ
resource "aws_subnet" "private_db" {
  count             = 3
  vpc_id            = aws_vpc.main.id
  cidr_block        = "10.0.\${count.index + 20}.0/24"
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = "private-db-subnet-\${count.index + 1}"
    Tier = "database"
  }
}

# NAT Gateway pour subnets privÃ©s
resource "aws_eip" "nat" {
  count  = 3
  domain = "vpc"

  tags = {
    Name = "nat-eip-\${count.index + 1}"
  }
}

resource "aws_nat_gateway" "main" {
  count         = 3
  allocation_id = aws_eip.nat[count.index].id
  subnet_id     = aws_subnet.public[count.index].id

  tags = {
    Name = "nat-gateway-\${count.index + 1}"
  }
}

# Route tables
resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = "public-route-table"
  }
}

resource "aws_route_table" "private" {
  count  = 3
  vpc_id = aws_vpc.main.id

  route {
    cidr_block     = "0.0.0.0/0"
    nat_gateway_id = aws_nat_gateway.main[count.index].id
  }

  tags = {
    Name = "private-route-table-\${count.index + 1}"
  }
}
\`\`\`

### alb.tf - Application Load Balancer

\`\`\`hcl
# ALB Security Group
resource "aws_security_group" "alb" {
  name        = "alb-sg"
  description = "Security group for ALB"
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# Application Load Balancer
resource "aws_lb" "main" {
  name               = "production-alb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.alb.id]
  subnets            = aws_subnet.public[*].id

  enable_deletion_protection = true
  enable_http2              = true

  tags = {
    Name = "production-alb"
  }
}

# Target Group
resource "aws_lb_target_group" "app" {
  name     = "app-target-group"
  port     = 3000
  protocol = "HTTP"
  vpc_id   = aws_vpc.main.id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    unhealthy_threshold = 10
    timeout             = 5
    interval            = 30
    path                = "/health"
    matcher             = "200"
  }

  deregistration_delay = 30

  tags = {
    Name = "app-tg"
  }
}

# Listener HTTP (redirect to HTTPS)
resource "aws_lb_listener" "http" {
  load_balancer_arn = aws_lb.main.arn
  port              = 80
  protocol          = "HTTP"

  default_action {
    type = "redirect"

    redirect {
      port        = "443"
      protocol    = "HTTPS"
      status_code = "HTTP_301"
    }
  }
}

# Listener HTTPS
resource "aws_lb_listener" "https" {
  load_balancer_arn = aws_lb.main.arn
  port              = 443
  protocol          = "HTTPS"
  ssl_policy        = "ELBSecurityPolicy-TLS-1-2-2017-01"
  certificate_arn   = aws_acm_certificate.main.arn

  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.app.arn
  }
}
\`\`\`

### asg.tf - Auto Scaling Group

\`\`\`hcl
# Launch Template
resource "aws_launch_template" "app" {
  name_prefix   = "app-lt-"
  image_id      = data.aws_ami.amazon_linux_2.id
  instance_type = "t3.medium"

  vpc_security_group_ids = [aws_security_group.app.id]

  user_data = base64encode(<<-EOF
              #!/bin/bash
              yum update -y
              yum install -y docker
              systemctl start docker
              systemctl enable docker

              # Run app container
              docker run -d \\
                -p 3000:3000 \\
                -e DB_HOST=\${aws_db_instance.main.address} \\
                -e DB_NAME=myapp \\
                -e DB_USER=admin \\
                myapp:latest
              EOF
  )

  iam_instance_profile {
    name = aws_iam_instance_profile.app.name
  }

  monitoring {
    enabled = true
  }

  tag_specifications {
    resource_type = "instance"
    tags = {
      Name = "app-instance"
    }
  }
}

# Auto Scaling Group
resource "aws_autoscaling_group" "app" {
  name                = "app-asg"
  vpc_zone_identifier = aws_subnet.private_app[*].id
  target_group_arns   = [aws_lb_target_group.app.arn]
  health_check_type   = "ELB"
  health_check_grace_period = 300

  min_size         = 2
  max_size         = 10
  desired_capacity = 3

  launch_template {
    id      = aws_launch_template.app.id
    version = "$Latest"
  }

  tag {
    key                 = "Name"
    value               = "app-asg-instance"
    propagate_at_launch = true
  }
}

# Auto Scaling Policies
resource "aws_autoscaling_policy" "scale_up" {
  name                   = "scale-up"
  scaling_adjustment     = 2
  adjustment_type        = "ChangeInCapacity"
  cooldown               = 300
  autoscaling_group_name = aws_autoscaling_group.app.name
}

resource "aws_autoscaling_policy" "scale_down" {
  name                   = "scale-down"
  scaling_adjustment     = -1
  adjustment_type        = "ChangeInCapacity"
  cooldown               = 300
  autoscaling_group_name = aws_autoscaling_group.app.name
}

# CloudWatch Alarms
resource "aws_cloudwatch_metric_alarm" "high_cpu" {
  alarm_name          = "high-cpu-utilization"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "CPUUtilization"
  namespace           = "AWS/EC2"
  period              = 120
  statistic           = "Average"
  threshold           = 80

  dimensions = {
    AutoScalingGroupName = aws_autoscaling_group.app.name
  }

  alarm_actions = [aws_autoscaling_policy.scale_up.arn]
}
\`\`\`

### rds.tf - Base de DonnÃ©es RDS

\`\`\`hcl
# RDS Subnet Group
resource "aws_db_subnet_group" "main" {
  name       = "main-db-subnet-group"
  subnet_ids = aws_subnet.private_db[*].id

  tags = {
    Name = "main-db-subnet-group"
  }
}

# RDS Security Group
resource "aws_security_group" "rds" {
  name        = "rds-sg"
  description = "Security group for RDS"
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port       = 5432
    to_port         = 5432
    protocol        = "tcp"
    security_groups = [aws_security_group.app.id]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# RDS PostgreSQL Multi-AZ
resource "aws_db_instance" "main" {
  identifier     = "production-db"
  engine         = "postgres"
  engine_version = "15.4"
  instance_class = "db.r6g.xlarge"

  allocated_storage     = 100
  max_allocated_storage = 1000
  storage_type          = "gp3"
  storage_encrypted     = true

  db_name  = "myapp"
  username = "admin"
  password = random_password.db_password.result

  multi_az               = true
  db_subnet_group_name   = aws_db_subnet_group.main.name
  vpc_security_group_ids = [aws_security_group.rds.id]

  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "mon:04:00-mon:05:00"

  enabled_cloudwatch_logs_exports = ["postgresql", "upgrade"]

  deletion_protection = true
  skip_final_snapshot = false
  final_snapshot_identifier = "production-db-final-snapshot"

  performance_insights_enabled = true

  tags = {
    Name = "production-database"
  }
}

# Read Replica
resource "aws_db_instance" "replica" {
  identifier             = "production-db-replica"
  replicate_source_db    = aws_db_instance.main.identifier
  instance_class         = "db.r6g.large"
  publicly_accessible    = false
  skip_final_snapshot    = true

  tags = {
    Name = "production-database-replica"
  }
}
\`\`\`

## ğŸš€ DÃ©ploiement

\`\`\`bash
# Initialize Terraform
terraform init

# Plan (voir les changements)
terraform plan -out=tfplan

# Apply (dÃ©ployer infrastructure)
terraform apply tfplan

# Outputs (rÃ©cupÃ©rer ALB URL, RDS endpoint, etc.)
terraform output
\`\`\`

## ğŸ“Š Monitoring et ObservabilitÃ©

### CloudWatch Dashboards

\`\`\`hcl
resource "aws_cloudwatch_dashboard" "main" {
  dashboard_name = "production-dashboard"

  dashboard_body = jsonencode({
    widgets = [
      {
        type = "metric"
        properties = {
          metrics = [
            ["AWS/ApplicationELB", "TargetResponseTime", { stat = "Average" }],
            [".", "RequestCount", { stat = "Sum" }],
            [".", "HTTPCode_Target_5XX_Count", { stat = "Sum" }]
          ]
          period = 300
          stat   = "Average"
          region = "eu-west-1"
          title  = "ALB Metrics"
        }
      },
      {
        type = "metric"
        properties = {
          metrics = [
            ["AWS/RDS", "CPUUtilization", { stat = "Average" }],
            [".", "DatabaseConnections", { stat = "Sum" }],
            [".", "ReadLatency", { stat = "Average" }],
            [".", "WriteLatency", { stat = "Average" }]
          ]
          period = 300
          region = "eu-west-1"
          title  = "RDS Metrics"
        }
      }
    ]
  })
}
\`\`\`

## ğŸ”’ SÃ©curitÃ© et Best Practices

1. **Network Isolation** : 3 tiers de subnets (public, private app, private DB)
2. **Security Groups** : RÃ¨gles strictes, least privilege
3. **Encryption** : RDS encrypted at rest, SSL/TLS in transit
4. **Secrets Management** : AWS Secrets Manager pour credentials
5. **IAM Roles** : EC2 instances avec roles, pas de access keys
6. **WAF** : AWS WAF sur ALB pour protection applicative
7. **Backups** : RDS automated backups 7 jours + manual snapshots

## ğŸš¨ Disaster Recovery

### RTO/RPO

- **RTO (Recovery Time Objective)** : <30 minutes
- **RPO (Recovery Point Objective)** : <5 minutes

### ProcÃ©dure de Failover

\`\`\`bash
# RDS Multi-AZ failover automatique
# En cas de panne AZ primaire:
# 1. RDS dÃ©tecte la panne (60-120 secondes)
# 2. Promeut automatiquement standby replica
# 3. Met Ã  jour DNS endpoint
# 4. Applications reconnectent automatiquement

# ASG self-healing
# EC2 unhealthy instances automatiquement remplacÃ©es
\`\`\`

## ğŸ“ˆ ROI et MÃ©triques

### Avant Architecture 3-Tiers AWS
- â±ï¸ **Downtime** : 2-3 pannes/mois = 4h downtime
- ğŸ’° **CoÃ»t** : Serveurs sur-provisionnÃ©s 24/7
- ğŸ“ˆ **Scaling** : Manuel, 2-3 heures
- ğŸ”§ **Maintenance** : Nuits/weekends pour patches
- âŒ **DisponibilitÃ©** : 99.5% (43h downtime/an)

### AprÃ¨s Architecture 3-Tiers AWS
- âœ… **Downtime** : 0 pannes (Multi-AZ, autoscaling)
- ğŸ’° **CoÃ»t** : -40% (autoscaling, right-sizing)
- ğŸ“ˆ **Scaling** : Automatique, <5 minutes
- ğŸ”§ **Maintenance** : Zero-downtime (rolling updates)
- âœ… **DisponibilitÃ©** : 99.99% (52 minutes downtime/an)

### MÃ©triques Business
- **Capacity** : 10K â†’ 100K users simultanÃ©s
- **Response Time** : -60% (ALB, caching)
- **Costs** : -40% vs on-premise
- **Time to Market** : -85% (Terraform IaC)

## ğŸ”— Ressources

- [AWS Well-Architected Framework](https://aws.amazon.com/architecture/well-architected/)
- [AWS Reference Architectures](https://aws.amazon.com/architecture/)
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [AWS Auto Scaling Best Practices](https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-best-practices.html)`,
        read_time: 18
    },

    'gcp-cloud-run-serverless': {
        content: `# GCP Cloud Run : Serverless Containers Auto-Scalant

## ğŸ¯ Use Case : API REST avec Trafic Variable (0 Ã  10K req/s)

Votre API REST a un trafic trÃ¨s variable : 0 requÃªtes la nuit (2h-6h), pic Ã  10,000 requÃªtes/seconde en journÃ©e. Avec des serveurs classiques, vous payez 24/7 pour gÃ©rer les pics. Avec Cloud Run, vous payez uniquement pour les requÃªtes effectuÃ©es, et le scaling Ã  0 Ã©limine les coÃ»ts pendant les heures creuses.

**Contexte rÃ©el** : Une API de traitement d'images pour une app mobile. Trafic nul la nuit, 50K requÃªtes/heure en journÃ©e. Cloud Run scale automatiquement de 0 Ã  100 instances en quelques secondes.

## ğŸ“‹ Pourquoi Cloud Run ?

### Avantages

- âœ… **Scale to Zero** : 0â‚¬ quand inutilisÃ©
- âœ… **Auto-scaling** : 0 â†’ 1000 instances en <30 secondes
- âœ… **Pay-per-use** : Facturation Ã  la milliseconde
- âœ… **Fully Managed** : 0 infrastructure Ã  gÃ©rer
- âœ… **Any Language** : Supporte tout conteneur
- âœ… **HTTPS automatique** : Certificats SSL/TLS inclus

### vs Alternatives

| Feature | Cloud Run | GKE | Cloud Functions | App Engine |
|---------|-----------|-----|-----------------|------------|
| Containers | âœ… | âœ… | âŒ | âŒ |
| Scale to 0 | âœ… | âŒ | âœ… | âš ï¸ |
| Cold start | ~500ms | N/A | ~1s | ~2s |
| Max instance size | 8 vCPU 32GB | Unlimited | 8GB | 8GB |
| Management | Full | Partial | Full | Full |

## ğŸ³ Dockerfile OptimisÃ© pour Cloud Run

### Exemple Node.js

\`\`\`dockerfile
# Multi-stage build
FROM node:20-alpine AS builder

WORKDIR /app

# Install dependencies
COPY package*.json ./
RUN npm ci --only=production && npm cache clean --force

# Build application
COPY . .
RUN npm run build

# Production stage
FROM node:20-alpine

WORKDIR /app

# Copy only necessary files
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/dist ./dist
COPY --from=builder /app/package.json ./

# Cloud Run requires listening on $PORT
ENV PORT=8080
EXPOSE 8080

# Non-root user
USER node

# Health check endpoint required
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s \\
  CMD node -e "require('http').get('http://localhost:8080/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"

CMD ["node", "dist/index.js"]
\`\`\`

### Exemple Python FastAPI

\`\`\`dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Cloud Run port
ENV PORT=8080

# Non-root user
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# Run with Uvicorn
CMD exec uvicorn main:app --host 0.0.0.0 --port $PORT --workers 1
\`\`\`

## ğŸš€ DÃ©ploiement via gcloud CLI

### DÃ©ploiement Simple

\`\`\`bash
# Build et deploy en une commande
gcloud run deploy myapi \\
  --source . \\
  --platform managed \\
  --region europe-west1 \\
  --allow-unauthenticated

# Cloud Run build automatiquement l'image via Buildpacks ou Dockerfile
\`\`\`

### DÃ©ploiement AvancÃ© avec Options

\`\`\`bash
gcloud run deploy myapi \\
  --image gcr.io/my-project/myapi:latest \\
  --platform managed \\
  --region europe-west1 \\
  --allow-unauthenticated \\
  --min-instances 0 \\
  --max-instances 100 \\
  --concurrency 80 \\
  --cpu 2 \\
  --memory 1Gi \\
  --timeout 300 \\
  --port 8080 \\
  --set-env-vars "NODE_ENV=production,LOG_LEVEL=info" \\
  --set-secrets "DB_PASSWORD=db-password:latest" \\
  --cpu-throttling \\
  --execution-environment gen2 \\
  --ingress all \\
  --vpc-connector my-vpc-connector \\
  --service-account myapi-sa@my-project.iam.gserviceaccount.com
\`\`\`

**Explication des options** :
- `--min-instances 0` : Scale to zero pour Ã©conomiser
- `--max-instances 100` : Maximum 100 instances simultanÃ©es
- `--concurrency 80` : 80 requÃªtes par instance
- `--cpu 2` : 2 vCPU par instance
- `--memory 1Gi` : 1GB RAM par instance
- `--timeout 300` : Timeout 5 minutes (max)
- `--execution-environment gen2` : 2Ã¨me gÃ©nÃ©ration (plus rapide)
- `--vpc-connector` : AccÃ¨s VPC privÃ© (ex: Cloud SQL)

## ğŸ—ï¸ Terraform Deployment

### main.tf

\`\`\`hcl
# Enable required APIs
resource "google_project_service" "run_api" {
  service = "run.googleapis.com"
}

resource "google_project_service" "iam_api" {
  service = "iam.googleapis.com"
}

# Service Account pour Cloud Run
resource "google_service_account" "cloudrun_sa" {
  account_id   = "cloudrun-sa"
  display_name = "Cloud Run Service Account"
}

# IAM pour accÃ¨s Cloud SQL
resource "google_project_iam_member" "cloudsql_client" {
  project = var.project_id
  role    = "roles/cloudsql.client"
  member  = "serviceAccount:\${google_service_account.cloudrun_sa.email}"
}

# Cloud Run Service
resource "google_cloud_run_service" "api" {
  name     = "myapi"
  location = "europe-west1"

  template {
    spec {
      service_account_name = google_service_account.cloudrun_sa.email

      containers {
        image = "gcr.io/\${var.project_id}/myapi:latest"

        ports {
          container_port = 8080
        }

        resources {
          limits = {
            cpu    = "2"
            memory = "1Gi"
          }
        }

        env {
          name  = "NODE_ENV"
          value = "production"
        }

        env {
          name = "DB_PASSWORD"
          value_from {
            secret_key_ref {
              name = google_secret_manager_secret.db_password.secret_id
              key  = "latest"
            }
          }
        }

        # Cloud SQL connection
        env {
          name  = "DB_HOST"
          value = "/cloudsql/\${google_sql_database_instance.main.connection_name}"
        }
      }

      container_concurrency = 80
      timeout_seconds       = 300

      # Scale to zero
      scaling {
        min_instance_count = 0
        max_instance_count = 100
      }
    }

    metadata {
      annotations = {
        "autoscaling.knative.dev/maxScale"      = "100"
        "autoscaling.knative.dev/minScale"      = "0"
        "run.googleapis.com/cloudsql-instances" = google_sql_database_instance.main.connection_name
        "run.googleapis.com/cpu-throttling"     = "true"
        "run.googleapis.com/execution-environment" = "gen2"
      }
    }
  }

  traffic {
    percent         = 100
    latest_revision = true
  }

  autogenerate_revision_name = true

  depends_on = [google_project_service.run_api]
}

# Public access
resource "google_cloud_run_service_iam_member" "public_access" {
  service  = google_cloud_run_service.api.name
  location = google_cloud_run_service.api.location
  role     = "roles/run.invoker"
  member   = "allUsers"
}

# Output URL
output "cloud_run_url" {
  value = google_cloud_run_service.api.status[0].url
}
\`\`\`

## ğŸ”„ CI/CD avec GitHub Actions

\`\`\`yaml
name: Deploy to Cloud Run

on:
  push:
    branches:
      - main

env:
  PROJECT_ID: my-gcp-project
  SERVICE_NAME: myapi
  REGION: europe-west1

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - id: auth
      uses: google-github-actions/auth@v2
      with:
        credentials_json: \${{ secrets.GCP_SA_KEY }}

    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@v2

    - name: Configure Docker
      run: gcloud auth configure-docker

    - name: Build image
      run: |
        docker build -t gcr.io/$PROJECT_ID/$SERVICE_NAME:$GITHUB_SHA .

    - name: Push image to GCR
      run: |
        docker push gcr.io/$PROJECT_ID/$SERVICE_NAME:$GITHUB_SHA

    - name: Deploy to Cloud Run
      run: |
        gcloud run deploy $SERVICE_NAME \\
          --image gcr.io/$PROJECT_ID/$SERVICE_NAME:$GITHUB_SHA \\
          --platform managed \\
          --region $REGION \\
          --allow-unauthenticated \\
          --min-instances 0 \\
          --max-instances 100

    - name: Smoke test
      run: |
        URL=$(gcloud run services describe $SERVICE_NAME \\
          --region $REGION \\
          --format 'value(status.url)')

        curl -f $URL/health || exit 1
        echo "âœ… Deployment successful!"
\`\`\`

## ğŸ“Š Monitoring avec Cloud Monitoring

### MÃ©triques Importantes

\`\`\`bash
# Request count
gcloud monitoring metrics list \\
  --filter="metric.type:run.googleapis.com/request_count"

# Request latencies
gcloud monitoring metrics list \\
  --filter="metric.type:run.googleapis.com/request_latencies"

# Container instance count
gcloud monitoring metrics list \\
  --filter="metric.type:run.googleapis.com/container/instance_count"
\`\`\`

### Alertes CloudMonitoring

\`\`\`hcl
# Alert sur latence Ã©levÃ©e
resource "google_monitoring_alert_policy" "high_latency" {
  display_name = "High Latency Alert"
  combiner     = "OR"
  conditions {
    display_name = "Request latency > 1s"
    condition_threshold {
      filter          = "resource.type = \\"cloud_run_revision\\" AND metric.type = \\"run.googleapis.com/request_latencies\\""
      duration        = "60s"
      comparison      = "COMPARISON_GT"
      threshold_value = 1000
      aggregations {
        alignment_period   = "60s"
        per_series_aligner = "ALIGN_MEAN"
      }
    }
  }

  notification_channels = [google_monitoring_notification_channel.email.name]
}
\`\`\`

## ğŸ” SÃ©curitÃ© et Best Practices

### 1. Authentication avec IAM

\`\`\`bash
# Service privÃ© (authentification requise)
gcloud run deploy myapi \\
  --no-allow-unauthenticated

# Accorder accÃ¨s Ã  un service account
gcloud run services add-iam-policy-binding myapi \\
  --region europe-west1 \\
  --member="serviceAccount:caller-sa@project.iam.gserviceaccount.com" \\
  --role="roles/run.invoker"
\`\`\`

### 2. Appel avec Token IAM

\`\`\`bash
# Obtenir token
TOKEN=$(gcloud auth print-identity-token)

# Appeler service
curl -H "Authorization: Bearer $TOKEN" \\
  https://myapi-xxx-ew.a.run.app/api/protected
\`\`\`

### 3. Secrets avec Secret Manager

\`\`\`bash
# CrÃ©er secret
echo -n "my-secret-password" | gcloud secrets create db-password --data-file=-

# Accorder accÃ¨s au service account
gcloud secrets add-iam-policy-binding db-password \\
  --member="serviceAccount:cloudrun-sa@project.iam.gserviceaccount.com" \\
  --role="roles/secretmanager.secretAccessor"

# Utiliser dans Cloud Run
gcloud run deploy myapi \\
  --set-secrets="DB_PASSWORD=db-password:latest"
\`\`\`

## ğŸš¨ Troubleshooting

### Cold Starts Lents

**ProblÃ¨me** : PremiÃ¨re requÃªte aprÃ¨s scale to zero prend 2-3 secondes

**Solutions** :
\`\`\`bash
# 1. Utiliser min-instances pour garder au moins 1 instance warm
gcloud run services update myapi --min-instances 1

# 2. Optimiser l'image Docker (multi-stage, distroless)
# 3. Utiliser execution-environment gen2
gcloud run services update myapi \\
  --execution-environment gen2

# 4. PrÃ©charger dÃ©pendances au startup
\`\`\`

### Timeout 504

**ProblÃ¨me** : RequÃªtes longues timeout aprÃ¨s 60s

**Solutions** :
\`\`\`bash
# Augmenter timeout (max 60 min pour gen2)
gcloud run services update myapi --timeout 600

# Pour tasks longues, utiliser Cloud Tasks + Cloud Run
\`\`\`

### Out of Memory

**ProblÃ¨me** : Container killed avec exit code 137

**Solutions** :
\`\`\`bash
# Augmenter mÃ©moire
gcloud run services update myapi --memory 2Gi

# Monitoring mÃ©moire
gcloud monitoring time-series list \\
  --filter='metric.type="run.googleapis.com/container/memory/utilizations"'
\`\`\`

## ğŸ’° Optimisation des CoÃ»ts

### Calculateur de CoÃ»ts

**Exemple** : API avec 10M requÃªtes/mois, 200ms avg response time

\`\`\`
RequÃªtes :     10,000,000 Ã— $0.40/million    = $4
vCPU time :    10M Ã— 0.2s Ã— 2 vCPU Ã— $0.00002400 = $96
Memory time :  10M Ã— 0.2s Ã— 1GB Ã— $0.00000250  = $5
Total :        $105/mois

vs VM e2-standard-2 (2 vCPU, 8GB) 24/7 : $49 + $30 (LoadBalancer) = $79/mois
Mais VM ne scale pas automatiquement et coÃ»te mÃªme avec 0 trafic!
\`\`\`

### StratÃ©gies d'Optimisation

1. **Scale to zero la nuit** : Ã‰conomie 50% si trafic uniquement en journÃ©e
2. **Concurrency Ã©levÃ©e** : 80-100 requÃªtes par instance
3. **CPU throttling** : Enable si pas CPU-intensive
4. **Request timeout** : RÃ©duire pour Ã©viter instances bloquÃ©es
5. **Compression** : Gzip responses pour rÃ©duire bandwidth

## ğŸ“ˆ ROI et MÃ©triques

### Avant Cloud Run (VM traditionnelles)
- ğŸ’° **CoÃ»t** : $150/mois (VM 24/7 + Load Balancer)
- â±ï¸ **Scaling** : Manuel, 10-15 minutes
- ğŸ”§ **Maintenance** : Patches OS, updates, monitoring
- ğŸ“Š **Utilisation** : 20% (over-provisioning pour pics)
- âŒ **CoÃ»t nuit** : $50/mois gaspillÃ©s (0 trafic)

### AprÃ¨s Cloud Run
- ğŸ’° **CoÃ»t** : $105/mois (pay-per-use)
- â±ï¸ **Scaling** : Automatique, <30 secondes
- ğŸ”§ **Maintenance** : 0 (fully managed)
- ğŸ“Š **Utilisation** : 100% (scale to zero)
- âœ… **CoÃ»t nuit** : $0 (scale to zero)

### MÃ©triques Business
- **Costs** : -30% vs VM
- **Time to Market** : -90% (deploy en 2 min)
- **Ops Time** : -100% (no servers to manage)
- **Scalability** : 10x (auto-scale to 1000 instances)

## ğŸ“ Use Cases IdÃ©aux

âœ… **Parfait pour** :
- APIs REST/GraphQL
- Webhooks
- Backend for frontend (BFF)
- Microservices
- Batch jobs (avec Cloud Tasks)
- Scheduled tasks (avec Cloud Scheduler)

âŒ **Pas adaptÃ© pour** :
- Applications stateful (sessions)
- WebSockets longue durÃ©e
- Processing >60 min (utiliser Cloud Functions 2nd gen)
- GPU workloads (utiliser GKE)

## ğŸ”— Ressources

- [Cloud Run Documentation](https://cloud.google.com/run/docs)
- [Cloud Run Pricing Calculator](https://cloud.google.com/products/calculator)
- [Best Practices](https://cloud.google.com/run/docs/tips)
- [Cloud Run Button](https://github.com/GoogleCloudPlatform/cloud-run-button)`,
        read_time: 16
    }

    // Je vais continuer avec les autres articles...
};

// Fonction pour mettre Ã  jour tous les articles
async function enrichAllArticles() {
    console.log('\nğŸš€ ENRICHISSEMENT DE TOUS LES ARTICLES\n');
    console.log('='.repeat(80));

    let successCount = 0;
    let errorCount = 0;

    for (const [slug, data] of Object.entries(enrichedArticles)) {
        console.log(`\nğŸ“ Traitement: ${slug}`);
        console.log(`   Longueur: ${data.content.length} caractÃ¨res`);

        const { error } = await supabase
            .from('blog_posts')
            .update({
                content: data.content,
                read_time: data.read_time
            })
            .eq('slug', slug);

        if (error) {
            console.error(`   âŒ Erreur:`, error.message);
            errorCount++;
        } else {
            console.log(`   âœ… Enrichi avec succÃ¨s!`);
            successCount++;
        }
    }

    console.log('\n' + '='.repeat(80));
    console.log(`\nğŸ“Š RÃ‰SUMÃ‰:`);
    console.log(`   âœ… SuccÃ¨s: ${successCount}`);
    console.log(`   âŒ Erreurs: ${errorCount}`);
    console.log(`\nâœ¨ Traitement terminÃ©!\n`);
}

enrichAllArticles().catch(console.error);
